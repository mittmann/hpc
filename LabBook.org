# -*- org-export-babel-evaluate: nil -*-
#+TITLE: Arthur Krause's LabBook
#+AUTHOR: Arthur Krause
#+LATEX_HEADER: \usepackage[margin=2cm,a4paper]{geometry}
#+STARTUP: overview indent
#+TAGS: Arthur(A) Lucas(L) noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This document is in English.

* 2016-01-25 First entry (proper emacs configuration file)            :Lucas:

I recommend you use Arnaud's emacs configuration file, available here:
+ http://mescal.imag.fr/membres/arnaud.legrand/misc/init.php

* 2016-07-06 Kickoff meeting (discussion some ideas)           :Arthur:Lucas:

We talk about lots of things with the objective to define the IC topic
in the context of the HPC4E project. Arthur will write a paragraph
about the following topics:
- PAPI (hardware counters in general, tools to trace them)
- OpenMP specification (for shared memory parallel programming)
- MPI (for distributed/shared memory - message passing)
- StarPU (applications are written as a graph of tasks with
  dependencies)

Deadline: 08/07, meeting will take place at 3PM

* 2016-07-11 Text as requested by Lucas                              :Arthur:
Note: You can use M-q to limit yourself to 80-columns.

Hardware counters are special registers that track events in the
processor such as cache misses, branch prediction success rate,
instruction count and memory bandwidth. With this data, it's possible
to establish a relation between the performance of a program and the
architecture it is running on. There are various ways to extract their
data, the most common is through the perf command, but there are
others. PAPI provides an interface that groups the events in called
EventSets. With this it's possible to correlate performance drops with
cache misses or memory bandwith, for example, indicating a
bottleneck. Intel provides a C++ API called Performance Counter
Monitor (PCM). With PCM, the programmer is able to access hardware
counter values directly in the code. There are studies that show the
possibility to estimate the power used by the processor analyzing a
small subset of these hardware counters.

OpenMP is an API for multithreading. It allows the programmer to put
compiler directives inside the code signaling a section that should be
parallelized, with the advantages of being multiplataform and usable
on GPGPU. The Message Passing Interface (MPI) is a specification of
what a message passing interface should do and how it should be
implemented. There are implementations for nearly all HPC platforms
and the code is easily portable to any platform that supports
MPI. StarPU works like a scheduler that assigns tasks to the multiple
processing units available for the user (e.g. GPU and CPU) given that
the programmer has written code for that architecture. StarPU can use
different scheduling policies that takes into account different
metrics to make its decisions, such as performance or power.
* 2016-07-13 Suggestion of initial concepts exploration               :Lucas:
Techniques/Concepts to explore (suggestion):

- How to measure _hardware counters_ (in an Intel arch.)
- OpenMP: parallel for and task parallelism
- Write programs using OpenMP
  - Matrix multiplication (regular load)
  - Mandelbrot set (irregular load)
  - Heat transfer
- Learning about OpenMP (hands-on)
  - http://openmp.org/mp-documents/omp-hands-on-SC08.pdf

Other topics
- NUMA (a fat node)
- Consider learning about StarPU
  - http://starpu.gforge.inria.fr/
  - Task parallelism
- Literate programming
  - https://en.wikipedia.org/wiki/Literate_programming
  - Donald E. Knuth
* 2016-07-18 Implementing MM (Example of use of Org Mode for coding) :Arthur:

Here's the code of my MM application:

#+begin_src C :results output :session :exports both :tangle no
#include <stdio.h>
int main() { 
printf("oe");
return 0; }
#+end_src

#+RESULTS:
: oe

Tangle this file and compile like this:

#+begin_src sh :results output :session :exports both
gcc -o mm  mm.c  
#+end_src

#+RESULTS:

Now, execute this on machine =orion3=.

#+begin_src sh :results output :session :exports both :dir /ssh:orion2:~/
ls
#+end_src

#+RESULTS:
: misc

* 2016-07-18 Meeting with Arthur/Lucas                         :Arthur:Lucas:

HPC4E (Brazil-Europa)
- http://hpc4e.eu/
- Collaboration project

CMP134 - Introdução ao Processamento Paralelo e Distribuído
- https://moodle.inf.ufrgs.br/course/view.php?id=722

CMP134 at Bitbucket
- https://bitbucket.org/schnorr/cmp134

* 2016-07-19 Suggested problems implementation using OpenMP          :Arthur:
** Matrix Multiplication

Here's the Matrix Multiplication code that I've implemented using
OpenMP. It multiplies two square dynamically generated matrices of
doubles.  It's size can be adjusted by modifying SIZE. Each column or
row has SIZE elements. To test it yourself, start by tangling the MM
code.
  
#+begin_src C :results output :session :exports both :tangle no
#Include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <time.h>

#define SIZE 800

double mat1[SIZE][SIZE], mat2[SIZE][SIZE], mat3[SIZE][SIZE];

int main(int argc, char** argv)
{
        int row, col;
        double start_t;
        if (argc == 1)    omp_set_num_threads(1);
        else if (argc == 2)    omp_set_num_threads(atoi(argv[1]));
        else
        {
                puts("invalid amount of parameters");
                return -1;
        
        }

        srand(time(NULL));      
        for (row=0; row<SIZE; row++)
        {
                for(col=0; col<SIZE; col++)
                {
                        mat1[row][col] = (double)rand() + (double)rand()/(double)RAND_MAX;
                        mat2[row][col] = (double)rand() + (double)rand()/(double)RAND_MAX;
                }
        }
        start_t = omp_get_wtime();

        #pragma omp parallel
        { 
                int i, _col;
                #pragma omp for
                for (row=0; row<SIZE; row++)
                {
                        for(_col=0; _col<SIZE; _col++)
                        {
                                for (i=0; i<SIZE; i++)
                                        mat3[row][_col] += mat1[row][i] * mat2[i][_col];
                        }
                }
        }

        printf("%f", omp_get_wtime() - start_t);
}
#+end_src

#+RESULTS:

Then, run the following script to compile and test it. It runs the
program 5 times for each number of threads, from 1 to 4 and stores the
results in a .csv file. Each line contains the results for each number
of threads.

#+begin_src sh :results output :session :exports both
gcc -fopenmp mm.c
mv a.out mm
rm mmlog mmlog.csv 
for j in 1 2 3 4
do
 for i in 1 2 3 4 5
 do
     ./mm $j >> mmlog
     echo "," >> mmlog
 done
 ./mm $j >> mmlog
 cat mmlog | tr -d '\n' >> mmlog.csv
 rm mmlog
echo "" >> mmlog.csv
done
cat mmlog.csv
#+end_src

#+RESULTS:

Here are my results running in a Intel Core i5-4210U CPU

: 4.352094,4.362803,4.347984,4.288565,4.358691,4.308480
: 2.283323,2.296820,2.359130,2.300184,2.366097,2.302083
: 2.954205,2.466486,2.328566,2.723584,2.918361,2.289822
: 2.260569,2.251039,2.250062,2.240696,2.284846,2.280797
*** cpu specs
Arquitetura:           x86_64
CPU(s):                4
On-line CPU(s) list:   0-3
Thread(s) per núcleo  2
Núcleo(s) por soquete:2
Soquete(s):            1
Nó(s) de NUMA:        1
Model name:            Intel(R) Core(TM) i5-4210U CPU @ 1.70GHz
CPU max MHz:           2700,0000
CPU min MHz:           800,0000
cache de L1d:          32K
cache de L1i:          32K
cache de L2:           256K
cache de L3:           3072K
NUMA node0 CPU(s):     0-3
 
* 2016-07-20 Feedback on Krause's MM implementation                   :Lucas:

Arthur has presented his MM's parallel implementation here:
- [[*Matrix Multiplication][Matrix Multiplication]]

First, I give you some suggestions about the code:

1. Thinking about reproducibility, you should initialize your matrix
   always with the same values. So, instead of =srand(time(NULL));=, do
   something link =srand(0);=, using a constant value. Doing that, at
   every experimental replication you are sure you have the same
   scenario.
2. You probably noticed the long execution time to initialize your
   matrices. I suggest you to use the following function to generate
   "random" values:
   #+begin_src C :results output :session :exports both
   //next function has been found here:
   //http://stackoverflow.com/questions/26237419/faster-than-rand
   //all credits to the authors there
   static unsigned int g_seed = 0;
   static inline int fastrand()
   {
     g_seed = (214013*g_seed+2531011);
     return (g_seed>>16)&0x7FFF;
   }
   #+end_src
3. Your code should receive the matrix size as parameter. Keep
   allocating them in the data segment (as globals); use a upper bound
   size, make sure the argument is always smaller than that. Note that
   800 is considered to be small. The performance analysis of your
   program should contain size as factor.
4. Regarding the program compilation: use the =-O3= optimization flag to
   gcc; if you have time, use clang (another compiler) adopting the
   compiler as a factor for your performance analysis. Show how one
   compiler's code is faster than the other.

Then, I give some suggestions about the running script:

1. The output of your script should be a CSV in the following format:
   the first line is the header containing the name of the
   columns. The rest of the file has the measurements (one single
   measurement per line). Example of a file considering your scenario:
   #+BEGIN_EXAMPLE
   Time, Thread, Size, Compiler
   4.352094, 1, 800, gcc
   4.362803, 1, 800, gcc
   4.347984, 1, 800, gcc
   4.288565, 1, 800, gcc
   4.358691, 1, 800, gcc
   4.308480, 1, 800, gcc
   #+END_EXAMPLE
   It's okay to repeat the information.
2. You can write =for i in `seq 1 5`= instead of what you did.
3. Everything you echo within the for could be redirected to a file
   with a single command, like this:
   #+begin_src sh :results output :session :exports both
   for i in 1 2 3 4 5
   do
      ./mm $j
      echo ","
   done >> mmlog
   #+end_src
4. Your script should log the details of the experimental platform.
5. A problem with your script is that you do 5 replications in
   order. For instance with four threads: you do five replications in
   a row. You should randomize your experimental design. The best way
   to do so is not do by yourself, and use R instead (see below a
   suggestion of design for your case).

Recommendations regarding the experimental setup + other topics:

1. I strongly recommend you to start reading the book of Raj Jain 1991: 
   *The Art of Computer Systems Performance Analysis*.
   The INF/UFRGS library has some units. Or google it.
2. Please, learn about experimental design in this book (full
   factorial and fractional designs to start with). It is of major
   importance since you provides you sound experimental data if you
   correctly apply them.
3. Tangle the experimental script (but do not commit the tangled file).
4. Other random questions: there are three for loops: try to
   parallelize the others. Which for is the best to be parallelized?
   Try to use other OpenMP schedulers to see which one is best (you
   can use dynamic, static, guided). For the MM scenario, which one is
   best? Why?

_A Experimental Design for your MM Scenario_

This is very rough, post questions when in doubt.

In English:

- 10 replications
- 2 factors (matrix size and number of threads)
  - matrix size is 400, 800, 1600 (enrich if you want to)
  - number of threads is 1 (sequential), 2, 4, 8
    - Sequential should be the code compiled without =-fopenmp=
- outcomes are the execution time and name of the platform
- randomize experiments

So, in R:

#+begin_src R :results output :session *x*:exports both :tangle no
  require(DoE.base);
  MM_Scenario <- fac.design (
           nfactors=2,
           replications=10,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(3,4),
           factor.names=list(
                size=c(400, 800, 1600),
                threads=c(1, 2, 4, 8)));

  export.design(MM_Scenario,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time","platform"));
#+end_src

#+RESULTS:
#+begin_example
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths
 creating full factorial with 12 runs ...
#+end_example

You should have two files now:

#+begin_src sh :results output :session :exports both
ls MM_Scenario.*
#+end_src

#+RESULTS:
: MM_Scenario.csv
: MM_Scenario.rda

The first one has your experimental design, each line indicating an
experiment you should run with your MM implementation. Example:

#+begin_src sh :results output :session :exports both
head MM_Scenario.csv
#+end_src

#+RESULTS:
#+begin_example
"name","run.no.in.std.order","run.no","run.no.std.rp","size","threads","time","platform"
"1","5",1,"5.1","800","2","",""
"2","12",2,"12.1","1600","8","",""
"3","3",3,"3.1","1600","1","",""
"4","7",4,"7.1","400","4","",""
"5","10",5,"10.1","400","8","",""
"6","8",6,"8.1","800","4","",""
"7","6",7,"6.1","1600","2","",""
"8","1",8,"1.1","400","1","",""
"9","9",9,"9.1","1600","4","",""
#+end_example

You can see they are randomized and there is a bunch of columns.

What you should do:

- Create a bash script that takes the design CSV (=MM_Scenario.csv=) as
  input, and run your application with the correct factors'
  values. For the example above, the first execution should be with
  matrix size of 800, with 2 threads.
- The output of your script should be the exact values of the input
  CSV plus the execution time and the platform in the corresponding
  columns.

See this entry for an example of script:
- [[*2016-07-20 (Unrelated) Example of experimental bash script][2016-07-20 (Unrelated) Example of experimental bash script]]

* 2016-07-20 (Unrelated) Example of experimental bash script          :Lucas:
Below I show you another project we are playing with for another project:
- Post your questions when in doubt
#+begin_src sh :results output :session :exports both :tangle no
  #!/bin/bash

  # Screening_run.sh
  
  # For each line
  # get the name of the program (column 5)
  # get the input size (column 6)
  # Launch the experiment, get execution time
  # print the line, with the time.

  function usage()
  {
      echo "$0 <benchmark> <binary> <design> <unique> <low_freq> <high_freq> <cpus> <notdryrun>";
      echo "where <benchmark> is one of the supported benchmarks";
      echo "where <binary> is the string (the command) that will be executed";
      echo "where <unique> can be any unique identifier you might like";
  }

  # parameter handling
  # Check if the benchmark is supported
  BENCHMARK=$1
  if [ -n "$BENCHMARK" -a "$BENCHMARK" != "lulesh" -a "$BENCHMARK" != "graph500"  -a "$BENCHMARK" != "minife" ]; then
      usage;
      exit;
  fi

  BINARY=$2
  if [ -z "$BINARY" ]; then
      usage;
      echo "Common commands to be placed in binary:";
      echo "lulesh: \"./lulesh2.0 -i 1\"";
      echo "graph500: \"SKIP_VALIDATION=1 ./omp-csr -s 25\"";
      echo "minife: \"./miniFE.x nx=200\"";
      exit;
  fi

  DESIGN=$3
  if [ -z "$DESIGN" ]; then
      usage;
      exit;
  fi
  UNIQUE=$4
  if [ -z "$UNIQUE" ]; then
      usage;
      exit
  fi
  REDFST_LOW=$5
  if [ -z "$REDFST_LOW" ]; then
      usage;
      exit
  fi
  REDFST_HIGH=$6
  if [ -z "$REDFST_HIGH" ]; then
      usage;
      exit
  fi
  REDFST_CPUS=$7
  if [ -z "$REDFST_CPUS" ]; then
      usage;
      exit
  fi
  NOTDRYRUN=$8

  # Lulesh mapping
  declare -A regions
  if [ "$BENCHMARK" = "lulesh" ]; then
      regions[A]=1
      regions[B]=2
      regions[C]=3
      regions[D]=4
      regions[E]=5
      regions[F]=6
      regions[G]=7
      regions[H]=8
      regions[I]=9
      regions[J]=10
      regions[K]=11
      regions[L]=12
      regions[M]=13
      regions[N]=14
      regions[O]=15
      regions[P]=16
      regions[Q]=17
      regions[R]=18
      regions[S]=19
      regions[T]=20
      regions[U]=21
      regions[V]=22
      regions[X]=23
      regions[W]=24
      regions[Y]=25
      regions[Z]=26
      regions[AA]=27
      regions[BB]=28
      regions[CC]=29
      regions[DD]=30
  elif [ "$BENCHMARK" = "graph500" ]; then
      # Graph500 mapping
      regions[A]=0
      regions[B]=21
      regions[C]=22
      regions[D]=24
      regions[E]=25
      regions[F]=26
      regions[G]=27
      regions[H]=28
      regions[I]=29
      regions[J]=30
      regions[K]=32
      regions[L]=33
      regions[M]=34
      regions[N]=35
      regions[O]=37
      regions[P]=41
      regions[Q]=42
  elif [ "$BENCHMARK" = "minife" ]; then
      regions[A]=1
      regions[B]=2
      regions[C]=3
      regions[D]=4
      regions[E]=5
      regions[F]=6
      regions[G]=7
      regions[H]=8
      regions[I]=9
      regions[J]=10
      regions[K]=11
      regions[L]=12
      regions[M]=13
      regions[N]=14
      regions[O]=15
      regions[P]=16
      regions[Q]=17
      regions[R]=18
      regions[S]=19
      regions[T]=20
      regions[U]=21
      regions[V]=22
      regions[X]=23
      regions[Y]=24
      regions[W]=25
      regions[Z]=26
  fi

  HOSTNAME=`hostname`
  export OMP_PROC_BIND=TRUE

  #check if the MSR is loaded
  if [ `lsmod | grep msr | wc -l` -ne 1 ]; then
      echo "The =msr= module is not loaded. It should be."
      usage;
      exit;
  fi
  # disable turbo boost on all cpus
  # we'll do it for all cores because its easier, but
  # one core per cpu would make more sense
  for cpu in $(ls /sys/bus/cpu/devices|sed 's/.*cpu//'); do
    sudo wrmsr -p${cpu} 0x1a0 0x4000850089
    if [[ "0" = $(sudo rdmsr -p${cpu} 0x1a0 -f 38:38) ]]; then
      echo "Failed to disable turbo boost for cpu$cpu. Aborting."
      exit 1
    fi
  done

  #check if cpufreq-info is present
  if [ -z `which cpufreq-info` ]; then
      echo "The =cpufreq-info= tool should be available."
      usage;
      exit;
  fi

  #check if high/low limits are within bounds
  CPUFREQ_INFO=`cpufreq-info -p`
  echo "cpufreq-info -p informs \"$CPUFREQ_INFO\""
  CPUFREQ_LOW_LIMIT=`echo $CPUFREQ_INFO | cut -d" " -f1`
  CPUFREQ_HIGH_LIMIT=`echo $CPUFREQ_INFO | cut -d" " -f2`
  CPUFREQ_GOVERNOR=`echo $CPUFREQ_INFO | cut -d" " -f3`
  if [ $REDFST_LOW -gt $REDFST_HIGH ]; then
      echo "Low frequency $REDFST_LOW is higher than high frequency $REDFST_HIGH.";
      usage;
      exit;
  fi
  if [ $REDFST_LOW -lt $CPUFREQ_LOW_LIMIT ]; then
      echo "Low frequency $REDFST_LOW is lower than low limit ($CPUFREQ_LOW_LIMIT) informed by cpufreq-info.";
      usage;
      exit;
  fi
  if [ $REDFST_HIGH -gt $CPUFREQ_HIGH_LIMIT ]; then
      echo "High frequency $REDFST_HIGH is higher than high limit ($CPUFREQ_HIGH_LIMIT) informed by cpufreq-info.";
      usage;
      exit;
  fi
  if [ "$CPUFREQ_GOVERNOR" != "userspace" ]; then
      echo "The cpufreq governor should be defined as \"userspace\", but is $CPUFREQ_GOVERNOR."
      usage;
      exit;
  fi


  #which factors are present in this design (get from first line)
  FACTORS=`head -n1 $DESIGN | cut -d"," -f5-21 | sed -e "s/\"//g" -e "s/,/ /g"`

  while read -r line; do
      NAME=`echo $line | cut -d"," -f1 | sed -e "s/\"//g" -e "s/,/ /g"`


      LEVELS=`echo $line | cut -d"," -f5-21 | sed -e "s/\"//g" -e "s/,/ /g"`
      COUNT=`echo $LEVELS | wc -w`
      #ignore first line
      if [[ $LEVELS =~ .*[[:alpha:]].* ]]; then
          #save first line of the design for later
          FIRSTDESIGNLINE="#G5K_REDFST#$line,unique,low,high,cpus,lowlimit,highlimit,governor,hostname"
          continue;
      fi

      echo
      echo "Starting experiment $NAME"
      echo "Starting at `date` on $HOSTNAME"
      echo

      #built LOW and HIGH variables
      REDFST_FASTREGIONS=
      REDFST_SLOWREGIONS=
      for i in `seq 1 $COUNT`; do
          FACTOR=`echo $FACTORS | cut -d" " -f$i`
          LEVEL=`echo $LEVELS | cut -d" " -f$i`
          if [[ $LEVEL =~ -1 ]]; then
              REDFST_SLOWREGIONS="${regions[$FACTOR]},$REDFST_SLOWREGIONS"
          elif [[ $LEVEL =~ 1 ]]; then
              REDFST_FASTREGIONS="${regions[$FACTOR]},$REDFST_FASTREGIONS"
          fi
      done
      #clean-up
      export REDFST_SLOWREGIONS=`echo $REDFST_SLOWREGIONS | sed "s/,$//"`
      export REDFST_FASTREGIONS=`echo $REDFST_FASTREGIONS | sed "s/,$//"`

      STDOUT=`tempfile`
      STDERR=`tempfile`
      REDFST=`tempfile`
      REDTRA=`tempfile`

      COMMAND="REDFST_FASTREGIONS=$REDFST_FASTREGIONS REDFST_SLOWREGIONS=$REDFST_SLOWREGIONS REDFST_LOW=$REDFST_LOW REDFST_HIGH=$REDFST_HIGH REDFST_CPUS=$REDFST_CPUS OMP_PROC_BIND=TRUE REDFST_HEADER=1 ${BINARY} 1> $STDOUT 2> $STDERR 3> $REDFST 4> $REDTRA"
      echo "COMMAND=\"$COMMAND\""
      if [ "$NOTDRYRUN" ]; then
          eval $COMMAND
      fi

      cat $STDOUT | sed "s/^/#STDOUT#/"
      cat $STDERR | sed "s/^/#STDERR#/"

      #output header
      echo "$FIRSTDESIGNLINE,`head -n1 $REDFST`"
      #output measurements
      cat $REDFST | tail -n+2 | sed "s/^/#REDFST#$line,$UNIQUE,$REDFST_LOW,$REDFST_HIGH,`echo $REDFST_CPUS | sed "s/,/-/g"`,$CPUFREQ_LOW_LIMIT,$CPUFREQ_HIGH_LIMIT,$CPUFREQ_GOVERNOR,$HOSTNAME,/"
      #output trace
      cat $REDTRA | sed "s/^/#REDTRA#/"

      rm -f $STDOUT $STDERR $REDFST $REDTRA

      echo
      echo "Finishing at `date` on $HOSTNAME"
      echo "Finishing experiment $NAME"
      echo

  done < $DESIGN

#+end_src
* 2016-07-20 How to avoid =org-babel-load-file= every time Emacs starts :Lucas:

Check:
- http://mescal.imag.fr/membres/arnaud.legrand/misc/init.html

Note that in the =Installation= section, you have the following lines:

Copy the =init.org= file to here =~/.emacs.d/init.org=.
http://mescal.imag.fr/membres/arnaud.legrand/misc/init.org

Once you did that, do =org-babel-load-file=. Provide the init.org file
that is within the =~/.emacs.d/= directory. When you do so, the loading
of the file creates another file called =init.el=. That file (with all
the special configurations from Arnaud) should be loaded automatically
next time you run emacs.

Let me know if that doesn't work for you.

* 2016-08-01 MM implementation following Lucas' suggested modifications :Arthur: 

This is the code from 2016-07-19 with the following modifications:
    - rand() is now replaced by fastrand() as the professor requested.
    - the matrix size is now a parameter received from the command line

#+begin_src C :results output :session :exports both :tangle no
    
#include <stdio.h>
#include <stdlib.h>
#include <omp.h>
#include <time.h>

#define MAX_SIZE 4096


double mat1[MAX_SIZE][MAX_SIZE], mat2[MAX_SIZE][MAX_SIZE], mat3[MAX_SIZE][MAX_SIZE];
static unsigned int g_seed = 0;
static inline int fastrand()
{
     g_seed = (214013*g_seed+2531011);
     return (g_seed>>16)&0x7FFF;
}


int main(int argc, char** argv)
{
        int row, col, size;
        double start_t;
        if (argc == 3) 
	{
		omp_set_num_threads(atoi(argv[1]));
		size = atoi(argv[2]);
		if (size > MAX_SIZE)
			{
				printf("Maximum matrix size is %d!", MAX_SIZE);
				return -1;
			}
	}
        else
        {
                puts("invalid amount of parameters");
                return -1;
        }

        for (row=0; row<size; row++)
        {
                for(col=0; col<size; col++)
                {
                        mat1[row][col] = (double)fastrand() + (double)fastrand()/(double)RAND_MAX;
                        mat2[row][col] = (double)fastrand() + (double)fastrand()/(double)RAND_MAX;
                }
        }
        start_t = omp_get_wtime();

        #pragma omp parallel
        { 
                int i, _col;
                #pragma omp for
                for (row=0; row<size; row++)
                {
                        for(_col=0; _col<size; _col++)
                        {
                                for (i=0; i<size; i++)
                                        mat3[row][_col] += mat1[row][i] * mat2[i][_col];
                        }
                }
        }

        printf("%f", omp_get_wtime() - start_t);
}
    #+end_src
    
First, create a folder for the logs and test scripts if it doesn't
exist already

#+begin_src sh :results output :session :exports both 
#!/bin/sh
mkdir log
mkdir scripts
#+end_src

#+RESULTS:

Now, tangle the following script that will compile and run the program
with different thread amounts and matrix sizes and store the results

#+begin_src sh :results output :session :exports both :tangle no
#!/bin/sh
#!/bin/bash

gcc -fopenmp -O3 mmv1.c
mv a.out mmv1

echo "Time, Threads, Size, Compiler" > log/mmv1.csv

for i in seq `seq 1 150`
do

THREADS=$((RANDOM % 4))
SIZE=$((RANDOM % 3))

if [ "$THREADS" == "0" ]; then
        THREADS=1

elif [ "$THREADS" == "1" ]; then
        THREADS=2

elif [ "$THREADS" == "2" ]; then
        THREADS=4

elif [ "$THREADS" == "3" ]; then
        THREADS=8
fi


if [ "$SIZE" == "0" ]; then
        SIZE=400

elif [ "$SIZE" == "1" ]; then
        SIZE=800

elif [ "$SIZE" == "2" ]; then
        SIZE=1600
fi


        ./mmv1 $THREADS $SIZE
        echo ", $THREADS, $SIZE, gcc"
done >> log/mmv1.csv

lscpu > log/mmv1_platform.txt

#+end_src


And finally, let's run the script

#+BEGIN_SRC sh :results output :session :exports both
  chmod +x scripts/test_mmv1.sh
  cat scripts/test_mmv1.sh | bash
  cat log/mmv1.csv
#+END_SRC

#+RESULTS:

* 2016-08-09 Reading a bash file line by line, then splitting         :Lucas:

This can be used to read a CSV file (the one whose name is registered
in the environment variable =$DESIGN=) where lines are separated by
commas. You can adapt it to your own needs since this example is very
particular to one scenario.

#+begin_src sh :results output :session :exports both
 while read -r line; do
      LEVELS=`echo $line | cut -d"," -f5-21 | sed -e "s/\"//g" -e "s/,/ /g"`
      #ignore first line
      if [[ $LEVELS =~ .*[[:alpha:]].* ]]; then
          #save first line of the design for later
          FIRSTDESIGNLINE="#G5K_REDFST#$line,unique,low,high,cpus,lowlimit,highlimit,governor,hostname"
          continue;
      fi
  done < $DESIGN
#+end_src
* 2016-08-10 Note on how to install clang with openmp                :Arthur:

inside an empty directory:
$ git clone https://github.com/clang-omp/llvm
$ git clone https://github.com/clang-omp/compiler-rt llvm/projects/compiler-rt
$ git clone -b clang-omp https://github.com/clang-omp/clang lvm/tools/clang

go to the llvm/projects folder:
$ svn co http://llvm.org/svn/llvm-project/openmp/trunk openmp
$ cd ..
$ mkdir build && cd build
$ cmake ../llvm -DCMAKE_COMPILER=gcc> -DCMAKE_CXX_COMPILER=g++
$ make omp


add this to /etc/environment 
C_INCLUDE_PATH="/home/arthur/openmpclang/llvm/projects/openmp/runtime/exports/common/include"
CPLUS_INCLUDE_PATH="/home/arthur/openmpclang/llvm/projects/openmp/runtime/exports/common/include"
LIBRARY_PATH="/home/arthur/openmpclang/llvm/projects/openmp/runtime/exports/lin_32e/lib"

create a random .conf file inside //etc/ld.so.conf.d/ and paste this inside
<PATH TO THE LLVM DIRECTORY>/llvm/projects/openmp/runtime/exports/lin_32e/lib

$ sudo ldconfig

* 2016-08-10 MM implementation using R and a better bash script      :Arthur:
Use R to create a test scenario with 512, 724, 1024 and 1448 (each one
has double the number of elements) with 1, 2, 3, 4 and 8 threads (1
still compiles as a parallel program but with only one thread) and
running the version compiled with gcc or clang.

#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario <- fac.design (
           nfactors=3,
           replications=10,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(4,4,2),
           factor.names=list(
                size=c(512, 724, 1024, 1448),
                threads=c(1, 2, 4, 8),
                compiler=c("gcc","clang")
           ));

  export.design(MM_Scenario,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
#+begin_example
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths
 creating full factorial with 32 runs ...
#+end_example

The following script runs the test scenario that R created:

#+begin_src bash :results output :session :exports both :tangle no
#!/bin/bash
gcc -fopenmp -O3 mmv1.c
mv a.out mmv1_gcc
clang -fopenmp -O3 mmv1.c
mv a.out mmv1_clang
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 
do
        if [ "$f1" != "\"name\"" ]; then
                THREADS=${f6//\"/}
                SIZE=${f5//\"/}
                if [ "$f7" == "\"gcc\"" ]; then
                       TIME=$(./mmv1_gcc $THREADS $SIZE)
                else
                       TIME=$(./mmv1_clang $THREADS $SIZE)
                fi
                echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$TIME"
	  else 
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8"
        fi
done < MM_Scenario.csv > data/MM_output.csv

#+end_src

#+RESULTS:

* 2016-08-17 Plotting the results                                    :Arthur:
The following script will parse the test output for 32 different
combinations of tests and calculate the mean between the results

#+begin_src bash :results output :session :exports both :tangle no
#!/bin/bash
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 
do
        if [ "$f1" != "\"name\"" ]; then
                THREADS=${f6//\"/}
                SIZE=${f5//\"/}
		  COMPILER=${f7//\"/}
		  SET=${f2//\"/}
		  TIME=${f8//\"/}
		  TEST=${f4//\"/}

		  
		  if [[ $TEST =~ $SET.1$ ]]; then
		    TIMES[$SET]=$TIME
		  else
		      TEMP=${TIMES[$SET]}
		  
		      TIMES[$SET]=`echo $TEMP + $TIME | bc`
		   
                fi
               
	  else 
	      
        fi
done < MM_output.csv
for SET in `seq 1 32`
do
    TIMES[$SET]=`echo "scale=6; ${TIMES[$SET]}/10" | bc`
    echo ${TIMES[$SET]}
done
#+end_src

#+RESULTS:
#+begin_example
"name","run.no.in.std.order","run.no","run.no.std.rp","size","threads","compiler"
.334225
1.058921
4.160997
12.481441
.186360
.617012
2.421287
6.919256
.136020
.415052
1.464716
4.735039
.131582
.373843
1.533235
4.993605
.288431
.957739
3.984647
11.973237
.156525
.560240
2.290977
6.796627
.151007
.430680
1.607344
4.798753
.118542
.391772
1.657193
4.934918
#+end_example
* 2016-08-18 Feedback                                                 :Lucas:

- you should commit the file =MM_output.csv= to the repository
  - you may create a =data= directory for that
- you should not calculate the average by yourself using bash
  - use R and the =dplyr= package instead
  - it can be as easy as:
    #+begin_src R :results output :session :exports both
    df <- read.csv("measures.csv");
    #suppose first column has the timings
    m <- mean(df$8);
    m
    #+end_src

    #+RESULTS:
    : Error in file(file, "rt") : não é possível abrir a conexão
    : Além disso: Warning message:
    : In file(file, "rt") :
    :   não foi possível abrir o arquivo 'measures.csv': Arquivo ou diretório não encontrado
    : Erro: unexpected numeric constant in "    m <- mean(df$8"
    : [1] NA

- your =mm.c= is empty:
  #+begin_src sh :results output
  cat mm.c
  #+end_src

  #+RESULTS:
  : #include <stdio.h>
  : int main() { return 0; }

  - where is it?
- Where is the comparison of your =mm.c= compilation using gcc and clang?
- Get in touch with me more frequently (daily if necessary) to advance faster

* 2016-08-22 MM Perf. Analysis with different compilers        :Arthur:Lucas:
** Results with O3 optimization level
*** Understand the data
#+begin_src R :results output :session :exports both
df <- read.csv("data/MM_output.csv");
head(df);
#+end_src

#+RESULTS:
:   name run.no.in.std.order run.no run.no.std.rp size threads compiler     time
: 1    1                  14      1          14.1  724       8      gcc 0.331328
: 2    2                  32      2          32.1 1448       8    clang 3.861296
: 3    3                   8      3           8.1 1448       2      gcc 6.376965
: 4    4                  23      4          23.1 1024       2    clang 2.077688
: 5    5                  10      5          10.1  724       4      gcc 0.338524
: 6    6                  11      6          11.1 1024       4      gcc 1.215392

You also have played with the size.

*** Analysis of the results
#+begin_src R :results output graphics :file img/MM_output_O3.png :exports both :width 400 :height 400 :session
df <- read.csv("data/MM_output.csv");
library(dplyr);
k <- df %>% select(size, threads, compiler, time) %>% group_by(size, threads, compiler) %>%
                      summarize(N=n(), mean=mean(time), se=3*sd(time)/sqrt(n())) %>% as.data.frame();
library(ggplot2);
ggplot(k, aes(x=as.factor(threads), y=mean, color=compiler)) +
    geom_point() +
    geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2) +
    theme_bw(base_size = 22) +
    ylim(0,NA) +
    ylab ("Runtime (seconds)") +
    xlab ("Number of threads") +
    scale_color_discrete (name="Compiler") +
    theme(legend.position="top") + 
    facet_wrap(~size, scales="free_y");
#+end_src

#+RESULTS:
[[file:img/MM_output_O3.png]]


**** Your interpretation

 The time for execution reduces almost by half doubling the amount of
 threads until 4, wich is the numer of cores on the machine used for
 testing. With 8 threads, the time is greater than with 4 because
 there is no parallelism gained since there is only 4 cores, and there
 is more scheduler overhead with 8 than with 4 threads. The scheduler
 overhead is also the reason for why the time for 2 is a bit greater
 than half the time for 1 thread. The same thing happens for 4 and 2
 threads.
 
 The test also reveals a better performance on the program compiled
 with llvm rather than gcc with -O3 optimization flag set. The llvm
 compiled program also showed a smaller standar error in the execution
 time than its counterpart compiled with gcc.
**** TODO Reinterpret considering the different matrix sizes
- State "TODO"       from              [2016-09-14 Wed 07:00]
** Results with O0 (no optimization)
*** Preparation for the next experiments _with no optimizations_
 To check if llvm gained performance with a better optimization we can
 compile with no compiler optimization at all and check the results.
 
 For this:
#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario_O0 <- fac.design (
           nfactors=2,
           replications=10,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(4,2),
           factor.names=list(
                threads=c(1, 2, 4, 8),
                compiler=c("gcc","clang")
           ));

  export.design(MM_Scenario_O0,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
#+begin_example
Loading required package: DoE.base
Loading required package: grid
Loading required package: conf.design

Attaching package: ‘DoE.base’

The following objects are masked from ‘package:stats’:

    aov, lm

The following object is masked from ‘package:graphics’:

    plot.design

The following object is masked from ‘package:base’:

    lengths
 creating full factorial with 8 runs ...
#+end_example


#+begin_src bash :results output :session :exports both :tangle no
#!/bin/bash
gcc -fopenmp -O0 mmv1.c
mv a.out mmv1_O0_gcc
clang -fopenmp -O0 mmv1.c
mv a.out mmv1_O0_clang
while IFS="," read f1 f2 f3 f4 f5 f6 f7 
do
        if [ "$f1" != "\"name\"" ]; then
                COMPILER=${f6//\"/}
                THREADS=${f5//\"/}
                TIME=$(./mmv1_O0_$COMPILER $THREADS 1024)

                echo "$f1,$f2,$f3,$f4,$f5,$f6,$TIME"
	  else 
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7"
        fi
done < MM_Scenario_O0.csv > data/MM_output_O0.csv

rm mmv1_O0_gcc mmv1_O0_clang
#+end_src

#+RESULTS:
*** TODO Understand the measure data
- State "TODO"       from              [2016-09-14 Wed 07:03]
#+begin_src R :results output :session :exports both
df <- read.csv("data/MM_output_O0.csv");
head(df);
#+end_src

I don't understand why you have supressed the *size* parameters of your
DoE described here:
- [[*Preparation for the next experiments _with no optimizations_][Preparation for the next experiments _with no optimizations_]]

Please, create a new entry at the end of the LabBook with a DoE with
the size parameter (same sizes you have previously used for O3). Rerun
the experiments so we can not only compare within O0, but against O3
as well. It would be much better if your DoE had the optimization
level as a factor (you could have all the four levels: 0, 1, 2 and 3).


*** Analysis of the results
Since there is no size factor, I take it out. What was the size used
here?
#+begin_src R :results output :session :exports both
df <- read.csv("data/MM_output_O0.csv");
head(df);
library(dplyr);
k <- df %>% select(threads, compiler, time) %>% group_by(threads, compiler) %>%
                      summarize(N=n(), mean=mean(time), se=3*sd(time)/sqrt(n())) %>% as.data.frame();
k
#+end_src

#+RESULTS:
#+begin_example
  name run.no.in.std.order run.no run.no.std.rp threads compiler      time
1    1                   4      1           4.1       8      gcc  3.848389
2    2                   8      2           8.1       8    clang  3.056484
3    3                   2      3           2.1       2      gcc  7.556800
4    4                   7      4           7.1       4    clang  3.020419
5    5                   6      5           6.1       2    clang  6.105754
6    6                   5      6           5.1       1    clang 11.903840
  threads compiler  N      mean          se
1       1    clang 10 11.909524 0.003175097
2       1      gcc 10 14.856414 0.003287345
3       2    clang 10  6.104234 0.004152455
4       2      gcc 10  7.565980 0.016595808
5       4    clang 10  3.173850 0.130747388
6       4      gcc 10  4.069532 0.148892384
7       8    clang 10  3.164443 0.109269834
8       8      gcc 10  4.086618 0.175229557
#+end_example

#+begin_src R :results output graphics :file img/MM_output_O0.png :exports both :width 400 :height 400 :session
df <- read.csv("data/MM_output_O0.csv");
library(dplyr);
k <- df %>% select(threads, compiler, time) %>% group_by(threads, compiler) %>%
                      summarize(N=n(), mean=mean(time), se=3*sd(time)/sqrt(n())) %>% as.data.frame();
library(ggplot2);
ggplot(k, aes(x=as.factor(threads), y=mean, color=compiler)) +
    geom_point() +
    geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2) +
    theme_bw(base_size = 22) +
    ylim(0,NA) +
    ylab ("Runtime (seconds)") +
    xlab ("Number of threads") +
    scale_color_discrete (name="Compiler") +
    theme(legend.position="top");
#+end_src

#+RESULTS:
[[file:img/MM_output_O0.png]]

 The program compilated with llvm is still better than the one
 compiled with gcc for this MM implementation
* 2016-09-14 Next steps towards *Performance Regression Test*          :Lucas:
** New DoE to test your matrix multiplication
Factors:
- optimization: 0, 1, 2 and 3
- compiler: clang, gcc
- matrix size: small, medium, big, huge
- threads: 1, 2, 4, 8, 16, 32
Outcomes:
- execution time
- total number of cache misses (L2, L3)

Please, ask Matthias Diener to have access to the *turing* machine.
** Learn about Spack to do Performance Regression Test
http://spack.readthedocs.io/en/latest/

How to install:
#+begin_src sh :results output
git clone https://github.com/llnl/spack.git
source spack/share/spack/setup-env.sh
#+end_src

My idea is that besides testing the optimization level and the
compiler, you would also use the *compiler version*. With spack, this
becomes very easy since you can have multiple compiler versions
installed in your HOME directory (each compiler version has a slightly
different OpenMP implementation, so one would expect different
parallel performance). The next step is to include all compiler
versions in your FF DoE.
** What is your application?
Several possibilities:
- Your matrix multiplication (you can use for your first tests)
- A very well optimization matrix multiplication: =dgemm=
- Cholesky factorization =dpotrf= (included in the chameleon spack's package)
- Irregular applications (such as those in my Reppar paper)
  https://github.com/lfgmillani/reppar2016
** Conduct a state of the art investigation
Search for "performance regression test" for the applications listed previously.
* 2016-09-21 Testing the MM with variable optimization levels        :Arthur:

I had removed the size factor in the previous test because the llvm
version had better performance with all sizes and, in order to reduce
the running time, I opted to test only for 1024x1024 matrices because
I tought that would be enough to find out if the different
optimization was the reason for the shorter execution time.

But anyways, here is the DoE with size and optimization levels as
factors:
** DoE
#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario <- fac.design (
           nfactors=4,
           replications=20,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(4,4,4,2),
           factor.names=list(
                size=c(512, 724, 1024, 1448),
                threads=c(1, 2, 4, 8),
                optimization=c("O0", "O1", "O2", "O3"),
                compiler=c("gcc","clang")
           ));

  export.design(MM_Scenario,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 128 runs ...

** Script for testing
#+begin_src bash :results output :session :exports both :tangle no
#!/bin/bash
gcc -fopenmp -O0 mmv1.c
mv a.out mmv1_gcc_O0
gcc -fopenmp -O1 mmv1.c
mv a.out mmv1_gcc_O1
gcc -fopenmp -O2 mmv1.c
mv a.out mmv1_gcc_O2
gcc -fopenmp -O3 mmv1.c
mv a.out mmv1_gcc_O3
clang -fopenmp -O0 mmv1.c
mv a.out mmv1_clang_O0
clang -fopenmp -O1 mmv1.c
mv a.out mmv1_clang_O1
clang -fopenmp -O2 mmv1.c
mv a.out mmv1_clang_O2
clang -fopenmp -O3 mmv1.c
mv a.out mmv1_clang_O3
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 f9
do
        if [ "$f1" != "\"name\"" ]; then
                THREADS=${f6//\"/}
                SIZE=${f5//\"/}
		  OPTIMIZATION=${f7//\"/}
                if [ "$f7" == "\"gcc\"" ]; then
                       TIME=$(./mmv1_gcc_$OPTIMIZATION $THREADS $SIZE)
                else
                       TIME=$(./mmv1_clang_$OPTIMIZATION $THREADS $SIZE)
                fi
                echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$TIME"
	  else 
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9"
        fi
done < MM_Scenario.csv > data/MM_output.csv
rm -rf mmv1_gcc_*
rm -rf mmv1_clang_*

#+end_src

** Plotting the results

I'm not sure about how to plot these results. The more obvious for me are:

- y=time x=optimization, with fixed matrix size and thread amount,
  compare the execution time difference with compiling optimization
  for each compiler
- y=time x=threads, with fixed matrix size and one grahp for each
  optimization level, compare the performance gain with increased
  threads for each compiler with different optimization levels
 
Executing the second one:

#+begin_src R :results output :session :exports both
df <- read.csv("data/MM_output.csv");
head(df);
library(dplyr);
k <- df %>% select(threads, compiler,optimization, size, time) %>% group_by(threads, compiler, optimization, size) %>%
                      summarize(N=n(), mean=mean(time), se=3*sd(time)/sqrt(n())) %>% as.data.frame();
#+end_src

#+RESULTS:
#+begin_example
  name run.no.in.std.order run.no run.no.std.rp size threads optimization
1    1                  53      1          53.1  512       2           O3
2    2                  58      2          58.1  724       4           O3
3    3                  33      3          33.1  512       1           O2
4    4                  97      4          97.1  512       1           O2
5    5                  42      5          42.1  724       4           O2
6    6                  49      6          49.1  512       1           O3
  compiler     time
1      gcc 0.144429
2      gcc 0.314401
3      gcc 0.286600
4    clang 0.286353
5      gcc 0.345997
6      gcc 0.286512

Attaching package: ‘dplyr’

The following objects are masked from ‘package:stats’:

    filter, lag

The following objects are masked from ‘package:base’:

    intersect, setdiff, setequal, union
#+end_example

#+begin_src R :results output graphics :file img/MM_output.pdf :exports both :session
df <- read.csv("data/MM_output.csv");
library(dplyr);
k <- df %>% select(threads, compiler, optimization, size, time) %>% group_by(threads, compiler, optimization, size) %>%
                      summarize(N=n(), mean=mean(time), se=3*sd(time)/sqrt(n())) %>% as.data.frame();
library(ggplot2);
a <- k[k$size==1448,]
ggplot(a, aes(x=as.factor(threads), y=mean, color=compiler, group=optimization)) +
    geom_point() +
    geom_errorbar(aes(ymin=mean-se, ymax=mean+se), width=.2) +
    theme_bw(base_size = 14) +
    ylim(0,NA) +
    ylab ("Runtime (seconds)") +
    xlab ("Number of threads") +
    scale_color_discrete (name="Compiler") +
    facet_wrap(~optimization) +
    theme(legend.position="top");
#+end_src

#+RESULTS:
[[file:img/MM_output.pdf]]

 This graph was not effective in showing differences between the
 compilers, so let's try to plot the percentual differences between
 the execution times.

 I couldn't do it. Could you help me, Lucas?

* 2016-09-23 New DoE considering cache misses                        :Arthur: 
#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario_2 <- fac.design (
           nfactors=4,
           replications=15,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(4,6,4,2),
           factor.names=list(
                size=c(512, 1024, 2048, 4096),
                threads=c(1, 2, 4, 8, 16, 32),
                optimization=c("O0", "O1", "O2", "O3"),
                compiler=c("gcc","clang")
           ));

  export.design(MM_Scenario_2,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time", "L2.misses", "L3.misses"));
#+end_src

#+RESULTS:
:  creating full factorial with 192 runs ...


The initial idea to count cache misses is through the perf utility, as
the example below

The output here in the LabBook is not the same when run directly into
bash, though
#+begin_src bash :results output :session :exports both
perf stat -B -e cache-references,cache-misses,L1-dcache-load-misses,LLC-loads-misses,cycles,instructions,branches ./mmv1_gcc 1 1000
#+end_src

#+RESULTS:
: 13.837267

    


Performance counter stats for './mmv1_gcc 32 1600':

    63.107.326.309 L1-dcache-loads                                              [22,99%]
     8.406.134.775 L1-dcache-stores                                             [35,43%]
     4.117.398.031 L1-dcache-load-misses     #    6,52% of all L1-dcache hits   [47,87%]
         3.071.692 L1-dcache-store-misses                                       [49,87%]
     4.148.736.630 LLC-loads                                                    [45,90%]
         1.302.953 LLC-stores                                                   [10,11%]
        55.854.974 LLC-loads-misses          #    1,35% of all LL-cache hits    [10,27%]
           561.544 LLC-stores-misses                                            [10,53%]

       3,839903229 seconds time elapsed


The plan was to subtract the L1 misses from the LLC loads and stores
in order to get the number of L2 hits, the problem is that it yields a
negative number, so this data is obviously incorrect.

     
 
 Is this correct, Lucas? Could you point me to a better way of getting this data?

* 2016-09-28 Getting L2 and L3 misses with likwid                    :Arthur: 

These tests will run on the beagle1 machine, since turing is locked
today and Gabriel told me he couldn't make likiwd work on it anyways.
Hostname resolution often doesn't work for me here so I'm going to ssh
directly to the machine's IP address.

We need to try to get the IP until it works

#+BEGIN_SRC sh :results output :session *a*:
nslookup beagle1
#+END_SRC

#+RESULTS:
: Server:		127.0.1.1
: 53
: 
: ** server can't find beagle1: NXDOMAIN

After some investigation, I decided to try and use "L2 _RQSTS _MISS"
as the number of L2 cache misses and "MEM _LOAD _UOPS _RETIRED _L3
_MISS" for L3 misses, but I am not sure if these are what I think they are.

#+begin_src bash :results output :session :exports both :dir /ssh:aulapinroot@143.54.12.87:~/arthur
#!/bin/bash
likwid-perfctr -f -c N:0-31 -g L2CACHE -g L3CACHE ./mmv1 32 3600 | grep -E 'MEM_LOAD_UOPS_RETIRED_L3_MISS STAT|L2_RQSTS_MISS STAT' | awk '{temp=$7; getline; print temp ", " $7;}'
#+end_src

#+RESULTS:
: 30356471991, 173475

Next steps are to install clang/llvm in beagle1 and design a complete test.
* 2016-09-30 Note on problems found trying to run the tests on beagle1 :Arthur:
-There is no OpenMP for llvm installed

-Cmake version is too old to compile a local version of llvm/openmp
(it's 2.8.7, needs to be at leas 2.8.8)

-I have no privilege within the machine to update cmake

-GCC version is too old to compile an updated version of cmake (it's
4.6.3 while it needs to be at least 4.7)

* 2016-09-30 Testing the MM on beagle1 for time and cache misses     :Arthur: 

** DoE
Since it's not possible to get the execution time while running the
program through likwid, time and cache misses must be tested
independently, so the test output will be a factor in the DoE

#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario_3 <- fac.design (
           nfactors=5,
           replications=10,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(5,6,4,2,2),
           factor.names=list(
                size=c(512, 724, 1024, 1448, 2048),
                threads=c(1, 2, 4, 8, 16, 32),
                optimization=c("O0", "O1", "O2", "O3"),
                compiler=c("gcc","clang"),
                test=c("time","cache")
           ));

  export.design(MM_Scenario_3,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time", "L2.misses", "L3.misses"));
#+end_src

#+RESULTS:
:  creating full factorial with 480 runs ...


** Script
First we get the IP address 

#+BEGIN_SRC sh :results output :session *a*
until nslookup beagle1 > /dev/null; do :; done
nslookup beagle1
#+END_SRC

#+RESULTS:
: 
: $ $ Server:		127.0.1.1
: 53
: 
: Name:	beagle1.inf.ufrgs.br
: Address: 143.54.12.87


Then we send our DoE and code to beagle1

#+begin_src bash :results output :session :exports both
scp MM_Scenario_3.csv arthur.krause@143.54.12.87:~/mm3
scp mmv1.c arthur.krause@143.54.12.87:~/mm3
#+end_src

#+RESULTS:



Now the test itself

 #+begin_src bash :results output :session :exports both :tangle no :dir /ssh:arthur.krause@143.54.12.87:~/mm3
#!/bin/bash
for i in `seq 0 3`
do
gcc -fopenmp -O$i mmv1.c
mv a.out mmv1_gcc_O$i
#clang -fopenmp -O$i mmv1.c
#mv a.out mmv1_clang_O$i
done

while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
do
        if [ "$f1" != "\"name\"" ]; then #if its not the first line
                
	          SIZE=${f5//\"/}
	          THREADS=${f6//\"/}                
		  OPTIMIZATION=${f7//\"/}
		  COMPILER=${f8//\"/}
		  if [ $COMPILER == gcc ]; then ##### REMOVE THIS LINE WHEN LLVM IS PROPERLY INSTALLED
		      if [ "$f9" == "\"time\"" ]; then
			  TIME=$(./mmv1_$COMPILER\_$OPTIMIZATION $THREADS $SIZE)
			  echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$TIME,$f11,$f12"
		      else
			  CACHE=$(likwid-perfctr -f -c N:0-31 -g L2CACHE -g L3CACHE ./mmv1_$COMPILER\_$OPTIMIZATION $THREADS $SIZE | grep -E 'MEM_LOAD_UOPS_RETIRED_L3_MISS STAT|L2_RQSTS_MISS STAT' | awk '{temp=$7; getline; print temp "," $7;}')
			  echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$f10,$CACHE"
		      fi             
		  fi ##### REMOVE THIS LINE WHEN LLVM IS PROPERLY INSTALLED
        else 
	      #if its the first line just repeat it
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$f10,$f11,$f12" 
        fi
done < MM_Scenario_3.csv > data/MM_output_3.csv
rm -rf mmv1_gcc_*
rm -rf mmv1_clang_*

#+end_src

 #+RESULTS:

Getting the output

#+begin_src bash :results output :session :exports both
scp arthur.krause@143.54.12.87:~/mm3/MM_output_3.csv /data/MM_output_3.csv
#+end_src
 





** Plot
             
The following R code should calculate the mean values and standard
error for the measured data and agreggate it into a single data frame.

If it takes too long, close emacs and try again
            
#+begin_src R :results output :session *a* :exports both

library(dplyr)
df <- read.csv("data/MM_output_3.csv");
k <- df %>% select(size, threads, optimization, compiler, test, time, L2.misses, L3.misses) %>% 
     filter(df$test=='time') %>%
     group_by(size, threads,  optimization, compiler) %>%
     summarize(m.time=mean(time), m.time.se=3*sd(time)/sqrt(n())) %>%
     as.data.frame()

g <- df %>% select(size, threads, optimization, compiler, test, time, L2.misses, L3.misses) %>% 
     filter(df$test=='cache') %>%
     group_by(size, threads,  optimization, compiler) %>%
     summarize(m.L2=mean(L2.misses), m.L2.se=3*sd(L2.misses)/sqrt(n()), m.L3=mean(L3.misses), m.L3.se=3*sd(L3.misses)/sqrt(n())) %>%
     as.data.frame()


n <- inner_join(k,g)
head(n)



#+end_src

#+RESULTS:
#+begin_example
Joining, by = c("size", "threads", "optimization", "compiler")
  size threads optimization compiler    m.time  m.time.se      m.L2
1  512       1           O0      gcc 0.9629030 0.07202432 134772711
2  512       1           O1      gcc 0.5266230 0.03676328 136072475
3  512       1           O2      gcc 0.3069949 0.02263621 134961325
4  512       1           O3      gcc 0.2998780 0.02087993 134951994
5  512       2           O0      gcc 0.4992201 0.02836698 139113278
6  512       2           O1      gcc 0.2736952 0.01925606 136085827
       m.L2.se m.L3 m.L3.se
1     7623.454    0       0
2    78717.370    0       0
3    15862.689    0       0
4    13013.346    0       0
5 13004653.685    0       0
6    65651.034    0       0
#+end_example



There are a lot of factors and measures to analyze. Where to start?

Probably I will discard this and move on to the complete test with gcc
and clang. This will be useful to check how worse the program compiled
in turing will run in beagle1 in relation to the one compiled in the
same machine.



Descobrir por que o llvm da turing compila tão ineficiente
* 2016-10-10 Testing with GCC and CLANG                              :Arthur: 
** DoE
The DoE is the same from the last entry

** Test
#+BEGIN_SRC sh :results output :session *a*
until nslookup beagle1 > /dev/null; do :; done
nslookup beagle1
#+END_SRC

Sending the code and DoE to beagle1

#+begin_src bash :results output :session :exports both
scp MM_Scenario_3.csv arthur.krause@143.54.12.87:~/mm4
scp mmv1.c arthur.krause@143.54.12.87:~/mm4
#+end_src

#+RESULTS:

Programs need to be compiled locally with clang then sent to beagle1
and compiled directly in beagle1 for gcc. So:

#+BEGIN_SRC bash :results output :session *a*: 
#!/bin/bash
clang -fopenmp -O0 mmv1.c
mv a.out mmv1_clang_O0
clang -fopenmp -O1 mmv1.c
mv a.out mmv1_clang_O1
clang -fopenmp -O2 mmv1.c
mv a.out mmv1_clang_O2
clang -fopenmp -O3 mmv1.c
mv a.out mmv1_clang_O3

scp mmv1_clang_O* arthur.krause@143.54.12.87:~/mm4

#+END_SRC

#+RESULTS:
: 
: arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ arthur@krause:~/hpc$ mmv1_clang_O0                                   0%    0     0.0KB/s   --:-- ETAmmv1_clang_O0                                 100%   13KB  13.1KB/s   00:00
: 0     0.0KB/s   --:-- ETAmmv1_clang_O1                                 100%   13KB  13.1KB/s   00:00
: 0     0.0KB/s   --:-- ETAmmv1_clang_O2                                 100%   13KB  13.1KB/s   00:00
: 0     0.0KB/s   --:-- ETAmmv1_clang_O3                                 100%   13KB  13.1KB/s   00:00

#+begin_src bash :results output :session :exports both :tangle no :dir /ssh:arthur.krause@143.54.12.87:~/mm4
#!/bin/bash
gcc -fopenmp -O0 mmv1.c
mv a.out mmv1_gcc_O0
gcc -fopenmp -O1 mmv1.c
mv a.out mmv1_gcc_O1
gcc -fopenmp -O2 mmv1.c
mv a.out mmv1_gcc_O2
gcc -fopenmp -O3 mmv1.c
mv a.out mmv1_gcc_O3
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 f9 f10 f11 f12
do
        if [ "$f1" != "\"name\"" ]; then #if its not the first line
                
	          SIZE=${f5//\"/}
	          THREADS=${f6//\"/}                
		  OPTIMIZATION=${f7//\"/}
		  COMPILER=${f8//\"/}
		      if [ "$f9" == "\"time\"" ]; then
			  TIME=$(./mmv1_$COMPILER\_$OPTIMIZATION $THREADS $SIZE)
			  echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$TIME,$f11,$f12"
		      else
			  CACHE=$(likwid-perfctr -f -c N:0-31 -g L2CACHE -g L3CACHE ./mmv1_$COMPILER\_$OPTIMIZATION $THREADS $SIZE | grep -E 'MEM_LOAD_UOPS_RETIRED_L3_MISS STAT|L2_RQSTS_MISS STAT' | awk '{temp=$7; getline; print temp "," $7;}')
			  echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$f10,$CACHE"
		      fi             
        else 
	      #if its the first line just repeat it
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9,$f10,$f11,$f12" 
        fi
done < MM_Scenario_3.csv > data/MM_output_4.csv
rm -rf mmv1_gcc_*
rm -rf mmv1_clang_*

#+end_src

** managing the data
#+begin_src R :results output :exports both :session *A*

library(dplyr)
library(reshape2)
df <- read.csv("data/MM_output_4.csv");
t <- df %>% select(size, threads, optimization, compiler, test, time, L2.misses, L3.misses) %>% 
     filter(df$test=='time') %>%
     group_by(size, threads,  optimization, compiler) %>%
     summarize(m.time=mean(time), m.time.se=3*sd(time)/sqrt(n())) %>%
     as.data.frame()

c <- df %>% select(size, threads, optimization, compiler, test, time, L2.misses, L3.misses) %>% 
     filter(df$test=='cache') %>%
     group_by(size, threads,  optimization, compiler) %>%
     summarize(m.L2=mean(L2.misses), m.L2.se=3*sd(L2.misses)/sqrt(n()), m.L3=mean(L3.misses), m.L3.se=3*sd(L3.misses)/sqrt(n())) %>%
     as.data.frame()

tgcc <- t  %>% filter(t$compiler=='gcc') %>% as.data.frame()
tclang <- t  %>% filter(t$compiler=='clang') %>% as.data.frame()

cgcc <- c  %>% filter(t$compiler=='gcc') %>% as.data.frame()
cclang <- c  %>% filter(t$compiler=='clang') %>% as.data.frame()


j <- inner_join(t,c)

#form a data frame with one entry per combination of size threads and optimization
p <- j %>% select(size,threads,optimization) %>% filter(j$compiler=='gcc') %>% as.data.frame()
p$time <- ((((tgcc$m.time - tclang$m.time)*100)/tclang$m.time)) 
p$L2 <- ((((cgcc$m.L2 - cclang$m.L2)*100)/cclang$m.L2)) 
p$L3 <- ((((cgcc$m.L3 - cclang$m.L3)*100)/(cclang$m.L3)))
m <- melt(p, id.vars = c("size", "threads", "optimization"), measure.vars = c("time", "L2", "L3")) 
#+end_src

#+RESULTS:
: Joining, by = c("size", "threads", "optimization", "compiler")

Now we have a dataframe with the percentual differences between the measures.

We will generate one file per matrix size, with one graph for each
optimization level, comparing the percentual increase from gcc to
clang for the measures.

#+begin_src R :results output graphics :file img/mm4/512.png :exports both :width 1800 :height 800 :session *A* 
library(ggplot2);
 ggplot(m[m$size==512,], aes(x=as.factor(threads), y=value, color=variable)) +
theme_bw() +  
geom_point() +
  theme_bw() + xlab("Threads") + ylab("% increase in gcc from clang") +
   facet_wrap(~optimization);
#+end_src

#+RESULTS:
[[file:img/mm4/512.png]]

#+begin_src R :results output graphics :file img/mm4/724.png :exports both :width 1800 :height 800 :session *A* 
library(ggplot2);
 ggplot(m[m$size==724,], aes(x=as.factor(threads), y=value, color=variable)) +
theme_bw() +  
geom_point() +
  theme_bw() + xlab("Threads") + ylab("% increase in gcc from clang") +
   facet_wrap(~optimization);
#+end_src

#+RESULTS:
[[file:img/mm4/724.png]]

#+begin_src R :results output graphics :file img/mm4/1024.png :exports both :width 1800 :height 800 :session *A* 
library(ggplot2);
 ggplot(m[m$size==1024,], aes(x=as.factor(threads), y=value, color=variable)) +
theme_bw() +  
geom_point() +
  theme_bw() + xlab("Threads") + ylab("% increase in gcc from clang") +
   facet_wrap(~optimization);
#+end_src

#+RESULTS:
[[file:img/mm4/1024.png]]

#+begin_src R :results output graphics :file img/mm4/1448.png :exports both :width 1800 :height 800 :session *A* 
library(ggplot2);
 ggplot(m[m$size==1448,], aes(x=as.factor(threads), y=value, color=variable)) +
theme_bw() +  
geom_point() +
  theme_bw() + xlab("Threads") + ylab("% increase in gcc from clang") +
   facet_wrap(~optimization);
#+end_src

#+RESULTS:
[[file:img/mm4/1448.png]]

#+begin_src R :results output graphics :file img/mm4/2048.png :exports both :width 1800 :height 800 :session *A* 
library(ggplot2);
 ggplot(m[m$size==2048,], aes(x=as.factor(threads), y=value, color=variable)) +
theme_bw() +  
geom_point() +
  theme_bw() + xlab("Threads") + ylab("% increase in gcc from clang") +
   facet_wrap(~optimization);
#+end_src

#+RESULTS:
[[file:img/mm4/2048.png]]

We can observe that for smaller matrices, both compilers performed
similarly.  On all optimization levels gcc had a great improvement in
relation to clang when the thread amount got to 32.

For size 724 matrices, there were cases where the clang program showed
L3 cache misses but the gcc one showed none.

The only cases where a compiler produced a program significanlty
better than the other were size=512 and optimization=O1 where clang
had less than a half the execution time from gcc with all thread
amount but 32, when it was faster, but showed no significant reduction
in L2 misses. With 8 and 32 threads, where the L2 misses amount was
even greater for gcc, it performed better, so it was probably not
because of this.

The only case where there was an observable difference in L2 misses
was with size=1024, but in this case even though gcc had way more L2
misses the execution time was noticeably shorter.

The conclusion is that there is a greater factor that dictates the
performance when comparing this particular application compiled with
gcc or clang/lvvm that is not cache misses, because there was no
observable correlation between L2 and L3 misses with execution time on
this experiment. It was noticeable that when run with 32 threads, gcc
was almost always better. It may be related to the fact that the
program versions compiled with gcc were compiled directly in the test
environment while the one compiled with clang was compiled in a
machine with only 4 cores and a *Uniform* memory access architecture. 

* 2016-10-21 GCC Matrix Multiplication Performance Regression Test   :Arthur:
I will test the performance of the same matrix multiplication for
different gcc versions, using the execution time as the metric.

The used versions will be the GCC 5 branch (5.1, 5.2, 5.3 and 5.4) and
the latest versions from latest branches (4.9.4 and 6.2.0).

The experiment factors will be: *Optimization Directive Level* (O0, O1,
O2 and O3); *Thread amount* (1, 2, 4, 8, 16 and 32), for one thread, a
sequential version will be run, not OpenMP with 1 thread as
previously; *Matrix dimension* (512, 724, 1024 and 1448), each level has
double the amount of elements as its predecessor. In order to make the
experiment take only a few hours and not multiple days to complete,
2048x2048 matrices won't be tested; and *Compiler Versions* as listed
above.

** Sequential mm implementation

#+begin_src C :results output :session :exports both :tangle no
#include <stdio.h>
#include <stdlib.h>
#include <time.h>
#include <sys/time.h>

#define MAX_SIZE 4096


double mat1[MAX_SIZE][MAX_SIZE], mat2[MAX_SIZE][MAX_SIZE], mat3[MAX_SIZE][MAX_SIZE];
static unsigned int g_seed = 0;
static inline int fastrand()
{
     g_seed = (214013*g_seed+2531011);
     return (g_seed>>16)&0x7FFF;
}
double GetTime(void)
{
   struct  timeval time;
   double  Time;

   gettimeofday(&time, (struct timezone *) NULL);
   Time = ((double)time.tv_sec*1000000.0 + (double)time.tv_usec);
   return(Time);
}

int main(int argc, char** argv)
{
        int row, col, size;
        double start_t;
        if (argc == 2)
        {
                size = atoi(argv[1]);
                if (size > MAX_SIZE)
                        {
                                printf("Maximum matrix size is %d!", MAX_SIZE);
                                return -1;
                        }
        }
        else
        {
                puts("invalid amount of parameters");
                return -1;
        }

        for (row=0; row<size; row++)
        {
                for(col=0; col<size; col++)
                {
                        mat1[row][col] = (double)fastrand() + (double)fastrand()/(double)RAND_MAX;
                        mat2[row][col] = (double)fastrand() + (double)fastrand()/(double)RAND_MAX;
                }
        }
        start_t = GetTime();

        {
                int i, _col;
                for (row=0; row<size; row++)
               {
                        for(_col=0; _col<size; _col++)
                        {
                                for (i=0; i<size; i++)
                                        mat3[row][_col] += mat1[row][i] * mat2[i][_col];
                        }
                }
        }

        printf("%f", (GetTime() - start_t)/1000000.0);
}

#+end_src

** Compilers instalation
The different GCC versions were installed using Spack
#+BEGIN_SRC sh :results output :session exports both :dir /ssh:beagle~/
spack install gcc@4.9.4
spack install gcc@5.1.0
spack install gcc@5.2.0
spack install gcc@5.3.0
spack install gcc@5.4.0
spack install gcc@6.2.0
#+END_SRC

Soft links for the executables were put in ~/compilers
#+BEGIN_SRC sh :results output :session exports both :dir /ssh:beagle~/
ls -l compilers
#+END_SRC

#+RESULTS:

** DoE
#+begin_src R :results output :session :exports both :tangle no
  require(DoE.base);
  MM_Scenario_5 <- fac.design (
           nfactors=4,
           replications=10,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(4,6,4,6),
           factor.names=list(
                size=c(512, 724, 1024, 1448),
                threads=c(1, 2, 4, 8, 16, 32),
                optimization=c("O0", "O1", "O2", "O3"),
                compiler=c("4.9.4","5.1.0","5.2.0","5.3.0","5.4.0","6.2.0")
           ));

  export.design(MM_Scenario_5,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 576 runs ...

** Script
#+BEGIN_SRC sh :results output :session *a*
until nslookup beagle1 > /dev/null; do :; done
nslookup beagle1
#+END_SRC

#+RESULTS:
: 
: $ Server:		127.0.1.1
: 53
: 
: Name:	beagle1.inf.ufrgs.br
: Address: 143.54.12.87

Sending the code and DoE to beagle1

#+begin_src bash :results output :session :exports both
scp MM_Scenario_5.csv arthur.krause@143.54.12.87:~/mm5
scp mmv1.c arthur.krause@143.54.12.87:~/mm5
scp mmv1_seq.c arthur.krause@143.54.12.87:~/mm5
#+end_src

#+RESULTS:


#+begin_src bash :results output :session :exports both :tangle no :dir /ssh:arthur.krause@143.54.12.87:~/mm5
#!/bin/bash

#compiles the program with different compilers and optimization levels
for i in `seq 0 3`
do
    for j in 4.9.4 5.1.0 5.2.0 5.3.0 5.4.0 6.2.0
	do
	    ../compilers/gcc-$j -fopenmp -O$i mmv1.c
	    mv a.out bin/mm_gcc-$j\_O$i

	    ../compilers/gcc-$j -O$i -o bin/mm_seq_gcc-$j\_O$i mmv1_seq.c
    done

done
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 f9
do
        if [ "$f1" != "\"name\"" ]; then #if its not the first line
                
	          SIZE=${f5//\"/}
	          THREADS=${f6//\"/}                
		  OPTIMIZATION=${f7//\"/}
		  VERSION=${f8//\"/}
		  if [ $THREADS != 1 ]; then
		      TIME=$(./bin/mm_gcc-$VERSION\_$OPTIMIZATION $THREADS $SIZE)
	          else
		      TIME=$(./bin/mm_seq_gcc-$VERSION\_$OPTIMIZATION $SIZE)
		  fi
		      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$TIME"       
        else 
	      #if its the first line just repeat it
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8,$f9" 
        fi
done < MM_Scenario_5.csv > data/MM_output_5.csv
#+end_src

#+RESULTS:

** Analysis
#+begin_src R :results output graphics :file img/mm5.1.png :session *mm5* :exports both :width 1920 :height 1080

library(dplyr)
df <- read.csv("data/MM_output_5.csv");
k <- df %>% select(size, threads, optimization, compiler, time) %>% 
     group_by(size, threads,  optimization, compiler) %>%
     summarize(m.time=mean(time), m.time.se=3*sd(time)/sqrt(n())) %>%
     as.data.frame()


library(ggplot2);
 ggplot(k[k$size==1448,], aes(x=as.factor(threads), y=m.time, color=compiler)) +
   theme_bw() +
   geom_point() +
   geom_errorbar(aes(ymin=m.time-m.time.se, ymax=m.time+m.time.se), width=.2) +
   facet_grid(~optimization);


#+end_src

#+RESULTS:
[[file:img/mm5.1.png]]

This plot is only conclusive to show that the O3 optimization in 6.2.0
brought a great performance gain comparing to other gcc versions, and
its not due to a better OpenMP implementation because the sequential
execution, which doesn't use OpenMP at all had an even more
significant performance gain.


#+begin_src R :results output :exports both :session *mm5* 
require(quantmod)
w <- k %>% filter(k$compiler == '5.1.0') %>% as.data.frame(); 
x <- k %>% filter(k$compiler == '5.2.0') %>% as.data.frame();
y <- k %>% filter(k$compiler == '5.3.0') %>% as.data.frame();
z <- k %>% filter(k$compiler == '5.4.0') %>% as.data.frame();
r <- k %>% select(size, threads, optimization) %>% filter(k$compiler == '5.1.0') %>% as.data.frame();

r$one <- -((w$m.time - x$m.time)*100)/w$m.time;
r$two <- -((x$m.time - y$m.time)*100)/x$m.time;
r$three <- -((y$m.time - z$m.time)*100)/y$m.time;

y <- k %>% filter(k$size == 1024) %>% as.data.frame();
sum (y$m.time)
#+end_src

#+RESULTS:
: [1] 1251.208

As we can see in the data frame there were some performance
regressions in the GCC 5 branch, but this needs to be better tested.

* 2016-10-24 Focusing the test on the GCC 5 branch                   :Arthur: 
The last test was too abrangent and took too long to run, so I will do
this next test with more repetitions and only for 1024x1024 matrices
in order to reduce the test time
** DoE
#+begin_src R :results output :session *mm6* :tangle no

 require(DoE.base);
  MM_Scenario_6 <- fac.design (
           nfactors=3,
           replications=20,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(6,4,4),
           factor.names=list(
                threads=c(1, 2, 4, 8, 16, 32),
                optimization=c("O0", "O1", "O2", "O3"),
                compiler=c("5.1.0","5.2.0","5.3.0","5.4.0")
           ));

  export.design(MM_Scenario_6,
                path=".",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 96 runs ...

** Test
#+BEGIN_SRC sh :results output :session *a*
until nslookup beagle1 > /dev/null; do :; done
nslookup beagle1
#+END_SRC

#+RESULTS:
: 
: $ Server:		127.0.1.1
: 53
: 
: Name:	beagle1.inf.ufrgs.br
: Address: 143.54.12.87

Sending the code and DoE to beagle1

#+begin_src bash :results output :session :exports both
scp MM_Scenario_6.csv arthur.krause@143.54.12.87:~/mm6
scp mmv1.c arthur.krause@143.54.12.87:~/mm6
scp mmv1_seq.c arthur.krause@143.54.12.87:~/mm6
#+end_src

#+RESULTS:


#+begin_src bash :results output :session :exports both :tangle no :dir /ssh:arthur.krause@143.54.12.87:~/mm6
#!/bin/bash
for i in `seq 0 3`
do
    for j in 5.1.0 5.2.0 5.3.0 5.4.0
	do
	    ../compilers/gcc-$j -fopenmp -O$i mmv1.c
	    mv a.out bin/mm_gcc-$j\_O$i

	    ../compilers/gcc-$j -O$i -o bin/mm_seq_gcc-$j\_O$i mmv1_seq.c
    done

done
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8 f9
do
        if [ "$f1" != "\"name\"" ]; then #if its not the first line
                
	          THREADS=${f5//\"/}                
		  OPTIMIZATION=${f6//\"/}
		  VERSION=${f7//\"/}
		  if [ $THREADS != 1 ]; then
		      TIME=$(./bin/mm_gcc-$VERSION\_$OPTIMIZATION $THREADS 1024)
	          else
		      TIME=$(./bin/mm_seq_gcc-$VERSION\_$OPTIMIZATION 1024)
		  fi
		      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$TIME"       
        else 
	      #if its the first line just repeat it
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8" 
        fi
done < MM_Scenario_6.csv > data/MM_output_6.csv
#+end_src

#+RESULTS:

* 2016-10-26 New look to the data ~MM_output_5.csv~                    :Lucas:
** Experiment
See the link below to understand how the experiment was conducted:
- [[*2016-10-21 GCC Matrix Multiplication Performance Regression Test][2016-10-21 GCC Matrix Multiplication Performance Regression Test]]
** Understand/summarize the data
#+begin_src R :results output :session :exports both
library(dplyr)
df <- read.csv("data/MM_output_5.csv");
dfk <- df %>% select(size, threads, optimization, compiler, time) %>% 
     group_by(size, threads,  optimization, compiler) %>%
     summarize(N=n(), m.time=mean(time), m.time.se=3*sd(time)/sqrt(n())) %>%
     as.data.frame();
head(dfk);
summary(dfk);
nrow(dfk);
#+end_src

#+RESULTS:
#+begin_example
  size threads optimization compiler  N    m.time    m.time.se
1  512       1           O0    4.9.4 10 0.9247332 0.0055336409
2  512       1           O0    5.1.0 10 0.8869819 0.0005567627
3  512       1           O0    5.2.0 10 0.8864826 0.0003773593
4  512       1           O0    5.3.0 10 0.8872129 0.0010723353
5  512       1           O0    5.4.0 10 0.8866873 0.0003755769
6  512       1           O0    6.2.0 10 0.8866870 0.0004175761
      size         threads     optimization  compiler        N     
 Min.   : 512   Min.   : 1.0   O0:144       4.9.4:96   Min.   :10  
 1st Qu.: 671   1st Qu.: 2.0   O1:144       5.1.0:96   1st Qu.:10  
 Median : 874   Median : 6.0   O2:144       5.2.0:96   Median :10  
 Mean   : 927   Mean   :10.5   O3:144       5.3.0:96   Mean   :10  
 3rd Qu.:1130   3rd Qu.:16.0                5.4.0:96   3rd Qu.:10  
 Max.   :1448   Max.   :32.0                6.2.0:96   Max.   :10  
     m.time          m.time.se        
 Min.   : 0.0295   Min.   :0.0000721  
 1st Qu.: 0.7287   1st Qu.:0.0095767  
 Median : 3.4530   Median :0.1134780  
 Mean   : 9.4401   Mean   :0.2930919  
 3rd Qu.:11.1502   3rd Qu.:0.3739422  
 Max.   :99.2315   Max.   :2.6035934
[1] 576
#+end_example
** Plot
#+begin_src R :results output graphics :file img/MM_output_5_time_per_version_for_32_threads.png :exports both :width 800 :height 800 :session
library(ggplot2);
 ggplot(k[k$threads == 32,], aes(x=compiler, y=m.time)) +
     geom_point() +
     geom_errorbar(aes(ymin=m.time-m.time.se, ymax=m.time+m.time.se), width=.2) +
     ylim (0, NA) +
     ylab ("Mean execution time [seconds]") +
     xlab ("Compiler Version") +
     theme_bw(base_size=28) +
     theme (
         legend.position = "top",
         axis.text.x = element_text (angle=45, hjust=1, size=18)
     ) +
    facet_grid(size~optimization, scale="free_y");
#+end_src

#+RESULTS:
[[file:img/MM_output_5_time_per_version_for_32_threads.png]]

Conclusion: version 6.2.0 provides an important reduction with O3.
** New DoEs (including clang)
Please, install gcc and clang with spack from here:
https://github.com/LLNL/spack
*** GCC
#+begin_src R :results output :session :exports both
require(DoE.base);

GCC_exp_0 <- fac.design (nfactors=4,
                                            nlevels=c(4,6,7,4),
                                            replications=10,
                                            randomize=TRUE,
                                            repeat.only=FALSE,
                                            seed=10373, blocks=1,
                                            factor.names=list( size=c(512, 724, 1024, 1448),
                                                              threads=c(1, 2, 4, 8, 16, 32),
                                                              version=c("4.9.4","5.1.0","5.2.0","5.3.0","5.4.0","6.1.0", "6.2.0"),
                                                              optimization=c("O0", "O1", "O2", "O3")));

export.design(GCC_exp_0, path="designs", filename=NULL, type="csv",
              replace=TRUE, response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 672 runs ...

*** CLANG
#+begin_src R :results output :session :exports both
require(DoE.base);

CLANG_exp_0 <- fac.design (nfactors=4,
                                            nlevels=c(4,6,7,4),
                                            replications=10,
                                            randomize=TRUE,
                                            repeat.only=FALSE,
                                            seed=10373, blocks=1,
                                            factor.names=list( size=c(512, 724, 1024, 1448),
                                                              threads=c(1, 2, 4, 8, 16, 32),
                                                              version=c("3.5.1", "3.6.2", "3.7.0", "3.7.1", "3.8.0", "3.8.1", "3.9.0"),
                                                              optimization=c("O0", "O1", "O2", "O3")));

export.design(CLANG_exp_0, path="designs", filename=NULL, type="csv",
              replace=TRUE, response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 672 runs ...

* 2016-11-01 Compiler performance regression test GCC vs CLang       :Arthur:
** Environment setup
The compilers were installed using Spack
(https://github.com/llnl/spack)

$ spack install <compiler>@<version> gcc versions 4.5.3, 4.6.4, 4.7.3
and 4.8.1 were already installed at /usr/bin

Soft links to the binaries were added to $HOME/compilers

#+begin_src bash :results output :session :exports both :dir /ssh:arthur.krause@turing:
ls -l compilers
#+end_src

#+RESULTS:
#+begin_example
total 48
lrwxrwxrwx 1 arthur.krause arthur.krause 125 Oct 26 20:16 clang-3.7.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/llvm-3.7.0-x7hanudrpab5gdtwtuami23qbgcxwuqp/bin/clang-3.7
lrwxrwxrwx 1 arthur.krause arthur.krause 125 Oct 26 20:17 clang-3.7.1 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/llvm-3.7.1-ashxj3khxg57woyl5nyzdwfzkawwwgfh/bin/clang-3.7
lrwxrwxrwx 1 arthur.krause arthur.krause 125 Oct 26 20:18 clang-3.8.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/llvm-3.8.0-kkybsxmnyeb57lah7ewwsvspck7zaqgh/bin/clang-3.8
lrwxrwxrwx 1 arthur.krause arthur.krause 125 Oct 26 20:19 clang-3.8.1 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/llvm-3.8.1-gcpkhvg52hkc4efo7tnnlxm6x56i2xtx/bin/clang-3.8
lrwxrwxrwx 1 arthur.krause arthur.krause 125 Oct 26 20:19 clang-3.9.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/llvm-3.9.0-a5qj3lsvguveqem7hspq6fh32r6yvwpd/bin/clang-3.9
lrwxrwxrwx 1 arthur.krause arthur.krause  16 Nov  1 10:33 gcc-4.5.3 -> /usr/bin/gcc-4.5
lrwxrwxrwx 1 arthur.krause arthur.krause  16 Nov  1 10:34 gcc-4.6.4 -> /usr/bin/gcc-4.6
lrwxrwxrwx 1 arthur.krause arthur.krause  16 Nov  1 10:34 gcc-4.7.3 -> /usr/bin/gcc-4.7
lrwxrwxrwx 1 arthur.krause arthur.krause  16 Nov  1 10:35 gcc-4.8.1 -> /usr/bin/gcc-4.8
lrwxrwxrwx 1 arthur.krause arthur.krause 118 Oct 26 20:16 gcc-4.9.4 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/gcc-4.9.4-6q3xieqeddm4sp24tu5nc4ajsa5ub5i7/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 116 Oct 26 20:15 gcc-5.1.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-4.8/gcc-5.1.0-7yexau7wrebs76g5b6oskseshc4b4jcj/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 116 Oct 26 20:15 gcc-5.2.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-4.8/gcc-5.2.0-q3m2ghd7oxn2hh5fym4s6d3tak6kw5pj/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 116 Oct 27 13:01 gcc-5.3.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-4.8/gcc-5.3.0-3tuujfkg6lubdmgdh452pvztfbpbrf2r/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 116 Oct 27 13:01 gcc-5.4.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-4.8/gcc-5.4.0-4fhv22qhx434aw5busqlxoj5d3h73tcg/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 118 Oct 26 21:02 gcc-6.1.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/gcc-6.1.0-lux7faf7ph2kk452cdohtgnnodwbccbf/bin/gcc
lrwxrwxrwx 1 arthur.krause arthur.krause 118 Oct 27 13:02 gcc-6.2.0 -> /home/arthur.krause/spack/opt/spack/linux-Ubuntu12-x86_64/gcc-5.4.0/gcc-6.2.0-z6feplsql27yu3scdj7qkgv5j75wru7e/bin/gcc
#+end_example

** DoE
#+begin_src R :results output :session :exports both

 require(DoE.base);
  perfreg <- fac.design (
           nfactors=3,
           replications=5,
           repeat.only=FALSE,
           blocks=1,
           randomize=TRUE,
           seed=10373,
           nlevels=c(7,3,16),
           factor.names=list(
                threads=c(1, 2, 4, 8, 16, 32, 64),
                optimization=c("O0", "O2", "O3"),
                compiler=c("clang-3.7.0", "clang-3.7.1", "clang-3.8.0", "clang-3.8.1", "clang-3.9.0", "gcc-4.5.3", "gcc-4.6.4", "gcc-4.7.3", "gcc-4.8.1", "gcc-4.9.4", "gcc-5.1.0", "gcc-5.2.0","gcc-5.3.0","gcc-5.4.0", "gcc-6.1.0", "gcc-6.2.0")
           ));

  export.design(perfreg,
                path="designs",
                filename=NULL,
                type="csv",
                replace=TRUE,
                response.names=c("time"));
#+end_src

#+RESULTS:
:  creating full factorial with 336 runs ...

** Test
*** Compiling the different versions of the program

#+begin_src bash :results output :session :exports both :dir /ssh:turing:~/exp/perfreg  
DIR=/home/arthur.krause/compilers
source ../../spack/share/spack/setup-env.sh
for i in `ls $DIR`
do
if [[ $i =~ clang* ]]; then
    VERSION=$(echo $i | cut -f2 -d "-")
    LB="-Wl,-rpath,"$(spack location -i llvm@$VERSION)/lib""
else
    LB=""
fi
    for j in O0 O2 O3
	do
	    
	     $DIR/$i -$j $LB -fopenmp src/mmv1.c -o bin/par_$i\_$j
	     $DIR/$i -$j src/mmv1_seq.c -o bin/seq_$i\_$j
	done
done
ls bin
#+end_src

#+RESULTS:
#+begin_example
par_clang-3.7.0_O0  par_gcc-4.8.1_O0  seq_clang-3.7.0_O0  seq_gcc-4.8.1_O0
par_clang-3.7.0_O2  par_gcc-4.8.1_O2  seq_clang-3.7.0_O2  seq_gcc-4.8.1_O2
par_clang-3.7.0_O3  par_gcc-4.8.1_O3  seq_clang-3.7.0_O3  seq_gcc-4.8.1_O3
par_clang-3.7.1_O0  par_gcc-4.9.4_O0  seq_clang-3.7.1_O0  seq_gcc-4.9.4_O0
par_clang-3.7.1_O2  par_gcc-4.9.4_O2  seq_clang-3.7.1_O2  seq_gcc-4.9.4_O2
par_clang-3.7.1_O3  par_gcc-4.9.4_O3  seq_clang-3.7.1_O3  seq_gcc-4.9.4_O3
par_clang-3.8.0_O0  par_gcc-5.1.0_O0  seq_clang-3.8.0_O0  seq_gcc-5.1.0_O0
par_clang-3.8.0_O2  par_gcc-5.1.0_O2  seq_clang-3.8.0_O2  seq_gcc-5.1.0_O2
par_clang-3.8.0_O3  par_gcc-5.1.0_O3  seq_clang-3.8.0_O3  seq_gcc-5.1.0_O3
par_clang-3.8.1_O0  par_gcc-5.2.0_O0  seq_clang-3.8.1_O0  seq_gcc-5.2.0_O0
par_clang-3.8.1_O2  par_gcc-5.2.0_O2  seq_clang-3.8.1_O2  seq_gcc-5.2.0_O2
par_clang-3.8.1_O3  par_gcc-5.2.0_O3  seq_clang-3.8.1_O3  seq_gcc-5.2.0_O3
par_clang-3.9.0_O0  par_gcc-5.3.0_O0  seq_clang-3.9.0_O0  seq_gcc-5.3.0_O0
par_clang-3.9.0_O2  par_gcc-5.3.0_O2  seq_clang-3.9.0_O2  seq_gcc-5.3.0_O2
par_clang-3.9.0_O3  par_gcc-5.3.0_O3  seq_clang-3.9.0_O3  seq_gcc-5.3.0_O3
par_gcc-4.5.3_O0    par_gcc-5.4.0_O0  seq_gcc-4.5.3_O0	  seq_gcc-5.4.0_O0
par_gcc-4.5.3_O2    par_gcc-5.4.0_O2  seq_gcc-4.5.3_O2	  seq_gcc-5.4.0_O2
par_gcc-4.5.3_O3    par_gcc-5.4.0_O3  seq_gcc-4.5.3_O3	  seq_gcc-5.4.0_O3
par_gcc-4.6.4_O0    par_gcc-6.1.0_O0  seq_gcc-4.6.4_O0	  seq_gcc-6.1.0_O0
par_gcc-4.6.4_O2    par_gcc-6.1.0_O2  seq_gcc-4.6.4_O2	  seq_gcc-6.1.0_O2
par_gcc-4.6.4_O3    par_gcc-6.1.0_O3  seq_gcc-4.6.4_O3	  seq_gcc-6.1.0_O3
par_gcc-4.7.3_O0    par_gcc-6.2.0_O0  seq_gcc-4.7.3_O0	  seq_gcc-6.2.0_O0
par_gcc-4.7.3_O2    par_gcc-6.2.0_O2  seq_gcc-4.7.3_O2	  seq_gcc-6.2.0_O2
par_gcc-4.7.3_O3    par_gcc-6.2.0_O3  seq_gcc-4.7.3_O3	  seq_gcc-6.2.0_O3
#+end_example

*** Testing

#+begin_src sh :results output :session :expors both
scp designs/perfreg.csv turing:~/exp/perfreg
#+end_src

#+RESULTS:

#+begin_src bash :results output :session :exports both :dir /ssh:turing:~/exp/perfreg  
#!/bin/bash
while IFS="," read f1 f2 f3 f4 f5 f6 f7 f8
do
        if [ "$f1" != "\"name\"" ]; then #if its not the first line
                
	          THREADS=${f5//\"/}                
		  OPTIMIZATION=${f6//\"/}
		  COMPILER=${f7//\"/}
		  if [ $THREADS != 1 ]; then
		      TIME=$(./bin/par_$COMPILER\_$OPTIMIZATION $THREADS 1300)
	          else
		      TIME=$(./bin/seq_$COMPILER\_$OPTIMIZATION 1300)
		  fi
		      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$TIME"       
        else 
	      #if its the first line just repeat it
	      echo "$f1,$f2,$f3,$f4,$f5,$f6,$f7,$f8" 
        fi
done < perfreg.csv > data/perfreg_out.csv
#

* 2017-01-28 Getting introduced to MPI                               :Arthur:

The tutorial found on
http://mpitutorial.com/ will be used.


** Hello World



First, tangle the hello world code below
kefile
: mpi_hello_world.c
#+begin_src C :results output :exports both :tangle codes/mpi_hello.c
#include <mpi.h>
#include <stdio.h>

int main(int argc, char** argv) {
    // Initialize the MPI environment
    MPI_Init(NULL, NULL);

    // Get the number of processes
    int world_size;
    MPI_Comm_size(MPI_COMM_WORLD, &world_size);

    // Get the rank of the process
    int world_rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);

    // Get the name of the processor
    char processor_name[MPI_MAX_PROCESSOR_NAME];
    int name_len;
    MPI_Get_processor_name(processor_name, &name_len);

    // Print off a hello world message
    printf("Hello world from processor %s, rank %d"
           " out of %d processors\n",
           processor_name, world_rank, world_size);

    // Finalize the MPI environment.
    MPI_Finalize();
}
#+end_src


Compile the code

#+begin_src sh :results output :exports both :dir codes
mpicc -o mpi_hello mpi_hello.c
#+end_src


#+RESULTS:

Then, run locally using mpirun, in this case with 4 processes

#+begin_src sh :results output :exports both :dir codes
mpirun -np 4 ./mpi_hello
#+end_src

#+RESULTS:
: Hello world from processor ana, rank 2 out of 4 processors
: Hello world from processor ana, rank 0 out of 4 processors
: Hello world from processor ana, rank 3 out of 4 processors
: Hello world from processor ana, rank 1 out of 4 processors


** Ping Pong

The code as seen at
https://github.com/wesleykendall/mpitutorial/blob/gh-pages/tutorials/mpi-send-and-receive/code/ping_pong.c

#+BEGIN_SRC C :tangle codes/mpi_pingpong.c
#include <mpi.h>
#include <stdio.h>
#include <stdlib.h>

int main(int argc, char** argv) {
  const int PING_PONG_LIMIT = 10;

  // Initialize the MPI environment
  MPI_Init(NULL, NULL);
  // Find out rank, size
  int world_rank;
  MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);
  int world_size;
  MPI_Comm_size(MPI_COMM_WORLD, &world_size);

  // We are assuming at least 2 processes for this task
  if (world_size != 2) {
    fprintf(stderr, "World size must be two for %s\n", argv[0]);
    MPI_Abort(MPI_COMM_WORLD, 1);
  }

  int ping_pong_count = 0;
  int partner_rank = (world_rank + 1) % 2;
  while (ping_pong_count < PING_PONG_LIMIT) {
    if (world_rank == ping_pong_count % 2) {
      // Increment the ping pong count before you send it
      ping_pong_count++;
      MPI_Send(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD);
      printf("%d sent and incremented ping_pong_count %d to %d\n",
             world_rank, ping_pong_count, partner_rank);
    } else {
      MPI_Recv(&ping_pong_count, 1, MPI_INT, partner_rank, 0, MPI_COMM_WORLD,
               MPI_STATUS_IGNORE);
      printf("%d received ping_pong_count %d from %d\n",
             world_rank, ping_pong_count, partner_rank);
    }
  }
  MPI_Finalize();
}
#+END_SRC

Compile

#+begin_src sh :results output :exports both :dir codes
mpicc -o mpi_pingpong mpi_pingpong.c
#+end_src

#+RESULTS:

Execute in 2 cores

#+begin_src sh :results output :exports both :dir codes
mpirun -np 2 ./mpi_pingpong
#+end_src

#+RESULTS:
#+begin_example
0 sent and incremented ping_pong_count 1 to 1
1 received ping_pong_count 1 from 0
0 received ping_pong_count 2 from 1
1 sent and incremented ping_pong_count 2 to 0
1 received ping_pong_count 3 from 0
1 sent and incremented ping_pong_count 4 to 0
0 sent and incremented ping_pong_count 3 to 1
0 received ping_pong_count 4 from 1
0 sent and incremented ping_pong_count 5 to 1
0 received ping_pong_count 6 from 1
0 sent and incremented ping_pong_count 7 to 1
1 received ping_pong_count 5 from 0
1 sent and incremented ping_pong_count 6 to 0
1 received ping_pong_count 7 from 0
1 sent and incremented ping_pong_count 8 to 0
1 received ping_pong_count 9 from 0
0 received ping_pong_count 8 from 1
0 sent and incremented ping_pong_count 9 to 1
0 received ping_pong_count 10 from 1
1 sent and incremented ping_pong_count 10 to 0
#+end_example

* 2017-02-11 Installing PAPI, Score-P                                :Arthur:

** PAPI

#+begin_src sh :results output :exports both
cd /tmp
wget http://icl.cs.utk.edu/projects/papi/downloads/papi-5.5.0.tar.gz
tar xvf papi-5.5.0.tar.gz
cd papi-5.5.0/src/
./configure
make
sudo make install
#+end_src

Verificamos que a instalação foi bem-sucedida com:

#+begin_src sh :results output :exports both
ls /usr/local/lib/ | grep -i papi
#+end_src

** Score-P

#+begin_src sh :results output :exports both
cd /tmp
wget http://www.vi-hps.org/upload/packages/scorep/scorep-3.0.tar.gz
tar vxf scorep-3.0.tar.gz
cd scorep-3.0/
mkdir build/
#for mpich3, if using openMPI, change the --with-mpi= part
./configure --prefix=$HOME/Programs/scorep-3.0/build/ --enable-papi --with-papi-lib=/usr/local/lib/ --with-papi-header=/usr/local/include/ --with-mpi=mpich3 --without-gui --without-shmen
make -j4 
sudo make install 
sed -i '1iexport PATH=$HOME/Programs/scorep-3.0/build/bin:$PATH' ~/.bashrc
sed -i '1iexport LD_LIBRARY_PATH=$HOME/Programs/scorep-3.0/build/lib:$LD_LIBRARY_PATH'  ~/.bashrc
#+end_src

* 2017-02-11 Tracing MPI ping pong                                   :Arthur: 

Intrumentation
#+begin_src sh :results output :exports both :dir codes
scorep mpicc -o mpi_pingpong_scorep mpi_pingpong.c
#+end_src

Execution
#+begin_src sh :results output :exports both :dir codes
rm -rf ../data/mpi_pingpong
export SCOREP_ENABLE_PROFILING=true 
export SCOREP_ENABLE_TRACING=true 
export SCOREP_TOTAL_MEMORY=3G 
export SCOREP_METRIC_PAPI=PAPI_L2_TCA,PAPI_L2_DCM 
export SCOREP_METRIC_RUSAGE=ru_utime,ru_stime
export SCOREP_EXPERIMENT_DIRECTORY=../data/mpi_pingpong
mpirun -np 2 ./mpi_pingpong_scorep
#+end_src

#+RESULTS:
#+begin_example
0 sent and incremented ping_pong_count 1 to 1
0 received ping_pong_count 2 from 1
1 received ping_pong_count 1 from 0
1 sent and incremented ping_pong_count 2 to 0
1 received ping_pong_count 3 from 0
1 sent and incremented ping_pong_count 4 to 0
0 sent and incremented ping_pong_count 3 to 1
0 received ping_pong_count 4 from 1
0 sent and incremented ping_pong_count 5 to 1
1 received ping_pong_count 5 from 0
0 received ping_pong_count 6 from 1
1 sent and incremented ping_pong_count 6 to 0
0 sent and incremented ping_pong_count 7 to 1
1 received ping_pong_count 7 from 0
0 received ping_pong_count 8 from 1
1 sent and incremented ping_pong_count 8 to 0
0 sent and incremented ping_pong_count 9 to 1
1 received ping_pong_count 9 from 0
1 sent and incremented ping_pong_count 10 to 0
0 received ping_pong_count 10 from 1
rank[0]:	 frequency = 2394465400/s 
rank[1]:	 frequency = 2394471583/s 
#+end_example

Assuming akypuera is installed and otf22paje was enabled upon
installation (see
https://github.com/schnorr/akypuera/wiki/OTF2WithAkypuera)

Converting to paje format
#+begin_src sh :results output :exports both :dir data/mpi_pingpong
otf22paje traces.otf2 > traces.paje
#+end_src

#+RESULTS:

Using pajeng to view the data as a csv
#+begin_src sh :results output :exports both :dir data/mpi_pingpong
pj_dump traces.paje > traces.csv
#+end_src

#+RESULTS:

#+begin_src sh :results output :exports both
less data/mpi_pingpong/traces.csv
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 0.015042, 0.015042, 0
Container, 0, machine, 0, 0.015042, 0.015042, Linux
Container, Linux, node, 0, 0.015042, 0.015042, ana
Container, ana, P, 0, 0.015042, 0.015042, MPI Rank 1
State, MPI Rank 1, STATE, 0.000000, 0.015042, 0.015042, 0.000000, main
State, MPI Rank 1, STATE, 0.000009, 0.014931, 0.014922, 1.000000, MPI_Init
State, MPI Rank 1, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 1, STATE, 0.014945, 0.014947, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 1, STATE, 0.014948, 0.014956, 0.000008, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014966, 0.014979, 0.000013, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014982, 0.014988, 0.000005, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014991, 0.014993, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014996, 0.014999, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015002, 0.015004, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015007, 0.015010, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015013, 0.015015, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015018, 0.015022, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015024, 0.015027, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015029, 0.015040, 0.000010, 1.000000, MPI_Finalize
Container, ana, P, 0, 0.015042, 0.015042, MPI Rank 0
State, MPI Rank 0, STATE, 0.010489, 0.015042, 0.004553, 0.000000, main
State, MPI Rank 0, STATE, 0.010497, 0.014924, 0.004427, 1.000000, MPI_Init
State, MPI Rank 0, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 0, STATE, 0.014945, 0.014946, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 0, STATE, 0.014948, 0.014953, 0.000005, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014965, 0.014981, 0.000015, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014984, 0.014987, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014990, 0.014993, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014996, 0.014999, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015001, 0.015005, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015008, 0.015010, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015013, 0.015016, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015019, 0.015021, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015024, 0.015027, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015030, 0.015039, 0.000009, 1.000000, MPI_Finalize
#+end_example

* 2017-02-13 Visualization of ping-pong traces                        :Lucas:
** Understanding the CSV file

Check here to understand the contents of the CSV file:
https://github.com/schnorr/pajeng/wiki/pj_dump

#+begin_src shell :results output
cat $(find data/mpi_pingpong/ | grep csv$)
#+end_src

#+RESULTS:
#+begin_example
Container, 0, 0, 0, 0.015042, 0.015042, 0
Container, 0, machine, 0, 0.015042, 0.015042, Linux
Container, Linux, node, 0, 0.015042, 0.015042, ana
Container, ana, P, 0, 0.015042, 0.015042, MPI Rank 1
State, MPI Rank 1, STATE, 0.000000, 0.015042, 0.015042, 0.000000, main
State, MPI Rank 1, STATE, 0.000009, 0.014931, 0.014922, 1.000000, MPI_Init
State, MPI Rank 1, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 1, STATE, 0.014945, 0.014947, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 1, STATE, 0.014948, 0.014956, 0.000008, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014966, 0.014979, 0.000013, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014982, 0.014988, 0.000005, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014991, 0.014993, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014996, 0.014999, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015002, 0.015004, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015007, 0.015010, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015013, 0.015015, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015018, 0.015022, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015024, 0.015027, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015029, 0.015040, 0.000010, 1.000000, MPI_Finalize
Container, ana, P, 0, 0.015042, 0.015042, MPI Rank 0
State, MPI Rank 0, STATE, 0.010489, 0.015042, 0.004553, 0.000000, main
State, MPI Rank 0, STATE, 0.010497, 0.014924, 0.004427, 1.000000, MPI_Init
State, MPI Rank 0, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 0, STATE, 0.014945, 0.014946, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 0, STATE, 0.014948, 0.014953, 0.000005, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014965, 0.014981, 0.000015, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014984, 0.014987, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014990, 0.014993, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014996, 0.014999, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015001, 0.015005, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015008, 0.015010, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015013, 0.015016, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015019, 0.015021, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015024, 0.015027, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015030, 0.015039, 0.000009, 1.000000, MPI_Finalize
#+end_example

We need only those lines starting with *State*, since they indicate the
behavior of each MPI rank along time. Let's get only those lines
then, saving to a temporary file.

#+begin_src shell :results output
cat $(find data/mpi_pingpong/ | grep csv$) | grep ^State > /tmp/tempfile.csv
cat /tmp/tempfile.csv
#+end_src

#+RESULTS:
#+begin_example
State, MPI Rank 1, STATE, 0.000000, 0.015042, 0.015042, 0.000000, main
State, MPI Rank 1, STATE, 0.000009, 0.014931, 0.014922, 1.000000, MPI_Init
State, MPI Rank 1, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 1, STATE, 0.014945, 0.014947, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 1, STATE, 0.014948, 0.014956, 0.000008, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014966, 0.014979, 0.000013, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014982, 0.014988, 0.000005, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.014991, 0.014993, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.014996, 0.014999, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015002, 0.015004, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015007, 0.015010, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015013, 0.015015, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015018, 0.015022, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 1, STATE, 0.015024, 0.015027, 0.000002, 1.000000, MPI_Send
State, MPI Rank 1, STATE, 0.015029, 0.015040, 0.000010, 1.000000, MPI_Finalize
State, MPI Rank 0, STATE, 0.010489, 0.015042, 0.004553, 0.000000, main
State, MPI Rank 0, STATE, 0.010497, 0.014924, 0.004427, 1.000000, MPI_Init
State, MPI Rank 0, STATE, 0.014939, 0.014943, 0.000004, 1.000000, MPI_Comm_rank
State, MPI Rank 0, STATE, 0.014945, 0.014946, 0.000002, 1.000000, MPI_Comm_size
State, MPI Rank 0, STATE, 0.014948, 0.014953, 0.000005, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014965, 0.014981, 0.000015, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014984, 0.014987, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.014990, 0.014993, 0.000004, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.014996, 0.014999, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015001, 0.015005, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015008, 0.015010, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015013, 0.015016, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015019, 0.015021, 0.000002, 1.000000, MPI_Send
State, MPI Rank 0, STATE, 0.015024, 0.015027, 0.000003, 1.000000, MPI_Recv
State, MPI Rank 0, STATE, 0.015030, 0.015039, 0.000009, 1.000000, MPI_Finalize
#+end_example
** Read data into R
#+begin_src R :results output :session :exports both
df <- read.csv("/tmp/tempfile.csv", header=FALSE, strip.white=TRUE);
names(df) <- c("Nature", "Rank", "Type", "Start", "End", "Duration", "Imbrication", "Value");
head(df);
#+end_src

#+RESULTS:
:   Nature       Rank  Type    Start      End Duration Imbrication         Value
: 1  State MPI Rank 1 STATE 0.000000 0.015042 0.015042           0          main
: 2  State MPI Rank 1 STATE 0.000009 0.014931 0.014922           1      MPI_Init
: 3  State MPI Rank 1 STATE 0.014939 0.014943 0.000004           1 MPI_Comm_rank
: 4  State MPI Rank 1 STATE 0.014945 0.014947 0.000002           1 MPI_Comm_size
: 5  State MPI Rank 1 STATE 0.014948 0.014956 0.000008           1      MPI_Recv
: 6  State MPI Rank 1 STATE 0.014966 0.014979 0.000013           1      MPI_Send

Only the Rank, Start, End and Value are of interest for us. Let's use
_dplyr_ verbs to select only those columns. Since Rank is a number,
let's wipe out the "MPI Rank" character of all values of the _Rank_
column, transforming it to integer. We also want to keep only MPI
states.
#+begin_src R :results output :session :exports both
library(dplyr);
df <- df %>%
    filter(grepl("MPI_", Value)) %>%
    select(Rank, Start, End, Value) %>%
    mutate(Rank = as.integer(gsub("MPI Rank ", "", Rank)));
head(df);
#+end_src

#+RESULTS:
:   Rank    Start      End         Value
: 1    1 0.000009 0.014931      MPI_Init
: 2    1 0.014939 0.014943 MPI_Comm_rank
: 3    1 0.014945 0.014947 MPI_Comm_size
: 4    1 0.014948 0.014956      MPI_Recv
: 5    1 0.014966 0.014979      MPI_Send
: 6    1 0.014982 0.014988      MPI_Recv

Great, let's check the types of each column:
#+begin_src R :results output :session :exports both
sapply(df, class);
#+end_src

#+RESULTS:
:      Rank     Start       End     Value 
: "integer" "numeric" "numeric"  "factor"

Perfect, let's move on.
** Plot
Let's use ggplot2.
#+begin_src R :results output graphics :file img/ping-pong-along-time_001.png :exports both :width 600 :height 200 :session
library(ggplot2);
df %>% ggplot() +
    theme_bw(base_size=12) +
    xlab("Time [s]") +
    ylab("Rank") +
    scale_fill_brewer(palette = "Set1") +
    theme (
#        plot.margin = unit(c(0,0,0,0), "cm"),
#        legend.margin = unit(.1, "line"),
        legend.position = "top",
        legend.title = element_blank()) +
    guides(fill = guide_legend(nrow = 1)) +
    geom_rect(aes(fill=Value,
                  xmin=Start,
                  xmax=End,
                  ymin=Rank,
                  ymax=Rank+0.9));
#+end_src

#+RESULTS:
[[file:img/ping-pong-along-time_001.png]]

We can see that most of the time the two ranks are in the =MPI_Init=
(violet color). So, What we could do instead is to remove this state
and take a look only to the others.

#+begin_src R :results output graphics :file img/ping-pong-along-time_002.png :exports both :width 600 :height 200 :session
library(ggplot2);
df %>% filter(Value != "MPI_Init") %>% ggplot() +
    theme_bw(base_size=12) +
    xlab("Time [s]") +
    ylab("Rank") +
    scale_fill_brewer(palette = "Set1") +
    theme (
#        plot.margin = unit(c(0,0,0,0), "cm"),
#        legend.margin = unit(.1, "line"),
        legend.position = "top",
        legend.title = element_blank()) +
    guides(fill = guide_legend(nrow = 1)) +
    geom_rect(aes(fill=Value,
                  xmin=Start,
                  xmax=End,
                  ymin=Rank,
                  ymax=Rank+0.9));
#+end_src

#+RESULTS:
[[file:img/ping-pong-along-time_002.png]]

Much more interesting now, we can even see the ping pong effect. The
white gaps can be considered computation. See how heteronegeous are
the durations of each operation.
